#!/usr/bin/env python3
"""
ç¬¬7å¤©ï¼šå°†RAGç³»ç»Ÿæ”¹é€ ä¸ºMCPæœåŠ¡å™¨
æä¾›æ–‡æ¡£ç®¡ç†ã€æ£€ç´¢å’Œé—®ç­”çš„MCPæ¥å£
"""

import asyncio
import os
import shutil
from typing import Any, Dict, List, Optional
from pathlib import Path

from mcp.server import Server, stdio_server
from mcp.server.models import InitializationOptions
from mcp.types import (
    Tool,
    Resource,
    TextContent,
    ResourceContents,
    TextResourceContents
)

# RAGç›¸å…³å¯¼å…¥
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.schema import Document

class RAGMCPServer:
    """
    RAGç³»ç»Ÿçš„MCPæœåŠ¡å™¨
    å°†å‘é‡æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½æš´éœ²ä¸ºMCPå·¥å…·
    """
    
    def __init__(self, persist_directory: str = "./mcp_chroma_db"):
        """
        åˆå§‹åŒ–RAG MCPæœåŠ¡å™¨
        
        å‚æ•°:
            persist_directory: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
        """
        self.server = Server("rag-system")
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.embeddings = None
        self.llm = None
        
        print(f"ğŸš€ åˆå§‹åŒ–RAG MCPæœåŠ¡å™¨")
        print(f"ğŸ“ æ•°æ®åº“ç›®å½•: {persist_directory}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        self._setup_handlers()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–RAGç»„ä»¶"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            print("ğŸ“š åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
            # åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“
            if os.path.exists(self.persist_directory):
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                print(f"âœ… åŠ è½½ç°æœ‰æ•°æ®åº“ï¼ŒåŒ…å« {self.vectorstore._collection.count()} ä¸ªæ–‡æ¡£")
            else:
                os.makedirs(self.persist_directory, exist_ok=True)
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                print("âœ… åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
            
            print("ğŸ¤– åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
            # åˆå§‹åŒ–LLMï¼ˆç”¨äºç”Ÿæˆç­”æ¡ˆï¼‰
            self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.1)
            print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _setup_handlers(self):
        """è®¾ç½®MCPå¤„ç†å™¨"""
        
        @self.server.list_tools()
        async def handle_list_tools() -> list[Tool]:
            """å®šä¹‰å¯ç”¨çš„RAGå·¥å…·"""
            return [
                Tool(
                    name="add_document",
                    description="æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "content": {
                                "type": "string",
                                "description": "æ–‡æ¡£å†…å®¹"
                            },
                            "metadata": {
                                "type": "object",
                                "description": "æ–‡æ¡£å…ƒæ•°æ®ï¼ˆå¦‚æ ‡é¢˜ã€æ¥æºã€æ ‡ç­¾ç­‰ï¼‰",
                                "default": {}
                            },
                            "chunk_size": {
                                "type": "integer",
                                "description": "æ–‡æœ¬åˆ†å—å¤§å°",
                                "default": 500
                            },
                            "chunk_overlap": {
                                "type": "integer",
                                "description": "åˆ†å—é‡å å¤§å°",
                                "default": 50
                            }
                        },
                        "required": ["content"]
                    }
                ),
                Tool(
                    name="add_file",
                    description="ä»æ–‡ä»¶æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "file_path": {
                                "type": "string",
                                "description": "æ–‡ä»¶è·¯å¾„"
                            },
                            "encoding": {
                                "type": "string",
                                "description": "æ–‡ä»¶ç¼–ç ",
                                "default": "utf-8"
                            },
                            "metadata": {
                                "type": "object",
                                "description": "é¢å¤–çš„å…ƒæ•°æ®",
                                "default": {}
                            }
                        },
                        "required": ["file_path"]
                    }
                ),
                Tool(
                    name="search",
                    description="åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æœç´¢æŸ¥è¯¢"
                            },
                            "k": {
                                "type": "integer",
                                "description": "è¿”å›ç»“æœæ•°é‡",
                                "default": 5
                            },
                            "filter": {
                                "type": "object",
                                "description": "å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶",
                                "default": None
                            },
                            "include_scores": {
                                "type": "boolean",
                                "description": "æ˜¯å¦åŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°",
                                "default": False
                            }
                        },
                        "required": ["query"]
                    }
                ),
                Tool(
                    name="answer_question",
                    description="åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "question": {
                                "type": "string",
                                "description": "è¦å›ç­”çš„é—®é¢˜"
                            },
                            "context_k": {
                                "type": "integer",
                                "description": "ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ–‡æ¡£æ•°é‡",
                                "default": 3
                            },
                            "include_sources": {
                                "type": "boolean",
                                "description": "æ˜¯å¦åœ¨å›ç­”ä¸­åŒ…å«ä¿¡æ¯æ¥æº",
                                "default": True
                            },
                            "temperature": {
                                "type": "number",
                                "description": "å›ç­”çš„åˆ›é€ æ€§ï¼ˆ0-1ï¼‰",
                                "default": 0.1
                            }
                        },
                        "required": ["question"]
                    }
                ),
                Tool(
                    name="list_documents",
                    description="åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ä¿¡æ¯",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "limit": {
                                "type": "integer",
                                "description": "è¿”å›çš„æ–‡æ¡£æ•°é‡é™åˆ¶",
                                "default": 10
                            },
                            "filter": {
                                "type": "object",\n",
                                "description": "å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶",
                                "default": None
                            }
                        }
                    }
                ),
                Tool(
                    name="delete_documents",
                    description="ä»çŸ¥è¯†åº“åˆ é™¤æ–‡æ¡£",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "filter": {
                                "type": "object",
                                "description": "åˆ é™¤æ¡ä»¶ï¼ˆåŸºäºå…ƒæ•°æ®ï¼‰"
                            },
                            "confirm": {
                                "type": "boolean",
                                "description": "ç¡®è®¤åˆ é™¤",
                                "default": False
                            }
                        },
                        "required": ["filter", "confirm"]
                    }
                ),
                Tool(
                    name="get_stats",
                    description="è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯",
                    inputSchema={
                        "type": "object",
                        "properties": {}
                    }
                )
            ]
        
        @self.server.list_resources()
        async def handle_list_resources() -> list[Resource]:
            """åˆ—å‡ºå¯ç”¨çš„èµ„æº"""
            return [
                Resource(
                    uri="rag://stats",
                    name="çŸ¥è¯†åº“ç»Ÿè®¡",
                    description="å½“å‰çŸ¥è¯†åº“çš„ç»Ÿè®¡ä¿¡æ¯",
                    mimeType="text/plain"
                ),
                Resource(
                    uri="rag://schema",
                    name="æ•°æ®åº“æ¨¡å¼",
                    description="å‘é‡æ•°æ®åº“çš„ç»“æ„ä¿¡æ¯",
                    mimeType="text/plain"
                )
            ]
        
        @self.server.read_resource()
        async def handle_read_resource(uri: str) -> ResourceContents:
            """è¯»å–èµ„æºå†…å®¹"""
            if uri == "rag://stats":
                stats = await self._get_detailed_stats()
                return TextResourceContents(
                    uri=uri,
                    mimeType="text/plain",
                    text=stats
                )
            elif uri == "rag://schema":
                schema = self._get_database_schema()
                return TextResourceContents(
                    uri=uri,
                    mimeType="text/plain",
                    text=schema
                )
            else:
                return TextResourceContents(
                    uri=uri,
                    mimeType="text/plain",
                    text=f"æœªçŸ¥çš„èµ„æº: {uri}"
                )
        
        @self.server.call_tool()
        async def handle_call_tool(
            name: str,
            arguments: Dict[str, Any]
        ) -> list[TextContent]:
            """å¤„ç†å·¥å…·è°ƒç”¨"""
            
            try:
                if name == "add_document":
                    result = await self._add_document(
                        arguments["content"],
                        arguments.get("metadata", {}),
                        arguments.get("chunk_size", 500),
                        arguments.get("chunk_overlap", 50)
                    )
                
                elif name == "add_file":
                    result = await self._add_file(
                        arguments["file_path"],
                        arguments.get("encoding", "utf-8"),
                        arguments.get("metadata", {})
                    )
                
                elif name == "search":
                    result = await self._search(
                        arguments["query"],
                        arguments.get("k", 5),
                        arguments.get("filter"),
                        arguments.get("include_scores", False)
                    )
                
                elif name == "answer_question":
                    result = await self._answer_question(
                        arguments["question"],
                        arguments.get("context_k", 3),
                        arguments.get("include_sources", True),
                        arguments.get("temperature", 0.1)
                    )
                
                elif name == "list_documents":
                    result = await self._list_documents(
                        arguments.get("limit", 10),
                        arguments.get("filter")
                    )
                
                elif name == "delete_documents":
                    result = await self._delete_documents(
                        arguments["filter"],
                        arguments.get("confirm", False)
                    )
                
                elif name == "get_stats":
                    result = await self._get_detailed_stats()
                
                else:
                    result = f"é”™è¯¯ï¼šæœªçŸ¥çš„å·¥å…· - {name}"\n",
                
                return [TextContent(type="text", text=result)]
                
            except Exception as e:
                return [TextContent(
                    type="text",
                    text=f"é”™è¯¯ï¼šæ‰§è¡Œå·¥å…· {name} æ—¶å‡ºé”™ - {str(e)}"
                )]
    
    async def _add_document(self, content: str, metadata: Dict, 
                          chunk_size: int, chunk_overlap: int) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        try:
            if not content.strip():
                return "é”™è¯¯ï¼šæ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©º"
            
            # æ–‡æœ¬åˆ†å‰²
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\\n\\n", "\\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
            
            chunks = text_splitter.split_text(content)
            
            if not chunks:
                return "é”™è¯¯ï¼šæ— æ³•åˆ†å‰²æ–‡æ¡£å†…å®¹"
            
            # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
            documents = []
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_index"] = i
                chunk_metadata["total_chunks"] = len(chunks)
                chunk_metadata["chunk_size"] = len(chunk)
                
                documents.append(Document(
                    page_content=chunk,
                    metadata=chunk_metadata
                ))\n",
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.vectorstore.add_documents(documents)
            
            # æŒä¹…åŒ–
            self.vectorstore.persist()
            
            return f"âœ… æˆåŠŸæ·»åŠ æ–‡æ¡£ï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—\\n" + \\\n",
                   f"ğŸ“Š å¹³å‡å—å¤§å°: {sum(len(c) for c in chunks) // len(chunks)} å­—ç¬¦\\n" + \\\n",
                   f"ğŸ“ å…ƒæ•°æ®: {metadata}"
            
        except Exception as e:
            return f"æ·»åŠ æ–‡æ¡£å¤±è´¥ï¼š{e}"
    
    async def _add_file(self, file_path: str, encoding: str, 
                       metadata: Dict) -> str:
        """ä»æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        try:
            path = Path(file_path)
            
            if not path.exists():
                return f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {file_path}"
            
            if not path.is_file():
                return f"é”™è¯¯ï¼š{file_path} ä¸æ˜¯æ–‡ä»¶"
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(path, 'r', encoding=encoding) as f:
                content = f.read()
            
            # æ·»åŠ æ–‡ä»¶ç›¸å…³çš„å…ƒæ•°æ®
            file_metadata = {
                "source": str(path),
                "filename": path.name,
                "file_extension": path.suffix,
                "file_size": path.stat().st_size,
                **metadata
            }
            
            # è°ƒç”¨æ·»åŠ æ–‡æ¡£æ–¹æ³•
            return await self._add_document(content, file_metadata, 500, 50)
            
        except UnicodeDecodeError as e:
            return f"æ–‡ä»¶ç¼–ç é”™è¯¯ï¼š{e}\\næç¤ºï¼šå°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç ï¼ˆå¦‚ gbk, latin1ï¼‰"
        except Exception as e:
            return f"æ·»åŠ æ–‡ä»¶å¤±è´¥ï¼š{e}"
    
    async def _search(self, query: str, k: int, filter: Optional[Dict], 
                     include_scores: bool) -> str:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            if not query.strip():
                return "é”™è¯¯ï¼šæœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
            
            # æ‰§è¡Œæœç´¢
            if include_scores:
                # å¸¦åˆ†æ•°çš„æœç´¢
                if filter:
                    results = self.vectorstore.similarity_search_with_score(
                        query, k=k, filter=filter
                    )
                else:
                    results = self.vectorstore.similarity_search_with_score(
                        query, k=k
                    )
                
                if not results:
                    return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                
                # æ ¼å¼åŒ–å¸¦åˆ†æ•°çš„ç»“æœ
                output = f"ğŸ” æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (æŸ¥è¯¢: '{query}'):\\n\\n"
                
                for i, (doc, score) in enumerate(results, 1):
                    output += f"[æ–‡æ¡£ {i}] (ç›¸ä¼¼åº¦: {score:.4f})\\n"
                    output += f"å†…å®¹: {doc.page_content[:200]}"
                    if len(doc.page_content) > 200:
                        output += "..."
                    output += f"\\nå…ƒæ•°æ®: {doc.metadata}\\n"
                    output += "-" * 60 + "\\n"
            else:
                # æ™®é€šæœç´¢
                if filter:
                    results = self.vectorstore.similarity_search(
                        query, k=k, filter=filter
                    )
                else:
                    results = self.vectorstore.similarity_search(query, k=k)
                
                if not results:
                    return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                
                # æ ¼å¼åŒ–ç»“æœ
                output = f"ğŸ” æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ (æŸ¥è¯¢: '{query}'):\\n\\n"
                
                for i, doc in enumerate(results, 1):
                    output += f"[æ–‡æ¡£ {i}]\\n"
                    output += f"å†…å®¹: {doc.page_content[:200]}"
                    if len(doc.page_content) > 200:
                        output += "..."
                    output += f"\\nå…ƒæ•°æ®: {doc.metadata}\\n"
                    output += "-" * 60 + "\\n"
            
            return output
            
        except Exception as e:
            return f"æœç´¢å¤±è´¥ï¼š{e}"
    
    async def _answer_question(self, question: str, context_k: int, 
                             include_sources: bool, temperature: float) -> str:
        """åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜"""
        try:
            if not question.strip():
                return "é”™è¯¯ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©º"
            
            # æœç´¢ç›¸å…³æ–‡æ¡£
            docs = self.vectorstore.similarity_search_with_score(
                question, k=context_k
            )
            
            if not docs:
                return "âŒ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            sources = []
            
            for i, (doc, score) in enumerate(docs, 1):
                context_parts.append(f"[ä¸Šä¸‹æ–‡ {i}]: {doc.page_content}")
                
                # æ”¶é›†æ¥æºä¿¡æ¯
                source_info = doc.metadata.get("source", f"æ–‡æ¡£å— {i}")
                if "filename" in doc.metadata:
                    source_info = doc.metadata["filename"]
                sources.append(f"{source_info} (ç›¸ä¼¼åº¦: {score:.3f})")
            
            context = "\\n\\n".join(context_parts)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç»™å‡ºå‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›è¯¦ç»†çš„å›ç­”ï¼š"""
            
            # è®¾ç½®æ¸©åº¦
            self.llm.temperature = temperature
            
            # ç”Ÿæˆç­”æ¡ˆ
            response = await self.llm.ainvoke(prompt)
            
            # æ„å»ºå®Œæ•´å›ç­”
            answer = f"ğŸ¤– **å›ç­”**\\n\\n{response.content}\\n\\n"
            
            if include_sources:
                answer += f"ğŸ“š **ä¿¡æ¯æ¥æº** (åŸºäº {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£):\\n"
                for i, source in enumerate(sources, 1):
                    answer += f"  {i}. {source}\\n"
            
            # æ·»åŠ æœç´¢ç»Ÿè®¡
            answer += f"\\nğŸ” **æ£€ç´¢ç»Ÿè®¡**\\n"
            answer += f"  - æŸ¥è¯¢é—®é¢˜: {question}\\n"
            answer += f"  - æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡: {len(docs)}\\n"
            answer += f"  - æœ€é«˜ç›¸ä¼¼åº¦: {docs[0][1]:.3f}\\n"
            answer += f"  - å›ç­”æ¸©åº¦: {temperature}\\n"
            
            return answer
            
        except Exception as e:
            return f"å›ç­”é—®é¢˜å¤±è´¥ï¼š{e}"
    
    async def _list_documents(self, limit: int, 
                            filter: Optional[Dict]) -> str:
        """åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£"""
        try:
            # è·å–æ–‡æ¡£
            collection = self.vectorstore._collection
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            query_params = {"limit": limit}
            if filter:
                query_params["where"] = filter
            
            results = collection.get(**query_params)
            
            if not results['documents']:
                return "çŸ¥è¯†åº“ä¸ºç©º"
            
            # åˆ†ææ–‡æ¡£
            documents = results['documents']
            metadatas = results['metadatas']
            
            output = f"ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨ (æ˜¾ç¤ºå‰ {len(documents)} ä¸ª):\\n\\n"
            
            # æŒ‰æºæ–‡ä»¶åˆ†ç»„
            source_groups = {}
            for i, (doc, metadata) in enumerate(zip(documents, metadatas)):
                source = metadata.get("source", metadata.get("filename", "æœªçŸ¥æ¥æº"))
                if source not in source_groups:
                    source_groups[source] = []
                source_groups[source].append((doc, metadata, i))
            
            # æ˜¾ç¤ºåˆ†ç»„ä¿¡æ¯
            for source, items in source_groups.items():
                output += f"ğŸ“„ **{source}** ({len(items)} ä¸ªæ–‡æ¡£å—)\\n"
                
                for doc, metadata, idx in items[:3]:  # æ¯ä¸ªæºæœ€å¤šæ˜¾ç¤º3ä¸ªå—
                    chunk_info = ""
                    if "chunk_index" in metadata:
                        chunk_info = f" [å— {metadata['chunk_index'] + 1}/{metadata.get('total_chunks', '?')}]"
                    
                    output += f"  {idx + 1}.{chunk_info} {doc[:100]}"
                    if len(doc) > 100:
                        output += "..."
                    output += "\\n"
                
                if len(items) > 3:
                    output += f"  ... è¿˜æœ‰ {len(items) - 3} ä¸ªå—\\n"
                
                output += "\\n"
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_chars = sum(len(doc) for doc in documents)
            avg_chars = total_chars // len(documents) if documents else 0
            
            output += f"ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯**\\n"
            output += f"  - æ€»æ–‡æ¡£å—æ•°: {len(documents)}\\n"
            output += f"  - ä¸åŒæ¥æºæ•°: {len(source_groups)}\\n"
            output += f"  - æ€»å­—ç¬¦æ•°: {total_chars:,}\\n"
            output += f"  - å¹³å‡å—å¤§å°: {avg_chars} å­—ç¬¦\\n"
            
            return output
            
        except Exception as e:
            return f"åˆ—å‡ºæ–‡æ¡£å¤±è´¥ï¼š{e}"
    
    async def _delete_documents(self, filter: Dict, confirm: bool) -> str:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            if not confirm:
                return "âŒ åˆ é™¤æ“ä½œéœ€è¦ç¡®è®¤ã€‚è¯·è®¾ç½® confirm=true"
            
            if not filter:
                return "âŒ åˆ é™¤æ“ä½œéœ€è¦æä¾›è¿‡æ»¤æ¡ä»¶ä»¥ç¡®ä¿å®‰å…¨"
            
            # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„æ–‡æ¡£
            collection = self.vectorstore._collection
            to_delete = collection.get(where=filter)
            
            if not to_delete['ids']:
                return f"æ²¡æœ‰æ‰¾åˆ°åŒ¹é…æ¡ä»¶çš„æ–‡æ¡£: {filter}"
            
            # æ‰§è¡Œåˆ é™¤
            collection.delete(where=filter)
            
            # æŒä¹…åŒ–
            self.vectorstore.persist()
            
            return f"âœ… æˆåŠŸåˆ é™¤ {len(to_delete['ids'])} ä¸ªæ–‡æ¡£\\n" + \\\n",
                   f"åˆ é™¤æ¡ä»¶: {filter}"
            
        except Exception as e:
            return f"åˆ é™¤æ–‡æ¡£å¤±è´¥ï¼š{e}"
    
    async def _get_detailed_stats(self) -> str:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection = self.vectorstore._collection
            results = collection.get()
            
            if not results['documents']:
                return "ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\\n\\nâŒ çŸ¥è¯†åº“ä¸ºç©º"
            
            documents = results['documents']
            metadatas = results['metadatas']
            
            # åŸºç¡€ç»Ÿè®¡
            total_docs = len(documents)
            total_chars = sum(len(doc) for doc in documents)
            avg_chars = total_chars // total_docs if total_docs > 0 else 0
            
            # æŒ‰æ¥æºç»Ÿè®¡
            sources = {}
            extensions = {}
            chunk_sizes = []
            
            for doc, metadata in zip(documents, metadatas):
                # æ¥æºç»Ÿè®¡
                source = metadata.get("source", metadata.get("filename", "æœªçŸ¥"))
                sources[source] = sources.get(source, 0) + 1
                
                # æ‰©å±•åç»Ÿè®¡
                if "file_extension" in metadata:
                    ext = metadata["file_extension"]
                    extensions[ext] = extensions.get(ext, 0) + 1
                
                # å—å¤§å°ç»Ÿè®¡
                chunk_sizes.append(len(doc))
            
            # æ„å»ºç»Ÿè®¡æŠ¥å‘Š
            stats = f"ğŸ“Š **çŸ¥è¯†åº“è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯**\\n\\n"
            
            # åŸºç¡€ä¿¡æ¯
            stats += f"ğŸ”¢ **åŸºç¡€ç»Ÿè®¡**\\n"
            stats += f"  - æ€»æ–‡æ¡£å—æ•°: {total_docs:,}\\n"
            stats += f"  - æ€»å­—ç¬¦æ•°: {total_chars:,}\\n"
            stats += f"  - å¹³å‡å—å¤§å°: {avg_chars} å­—ç¬¦\\n"
            stats += f"  - æœ€å¤§å—å¤§å°: {max(chunk_sizes)} å­—ç¬¦\\n"
            stats += f"  - æœ€å°å—å¤§å°: {min(chunk_sizes)} å­—ç¬¦\\n"
            stats += f"  - æ•°æ®åº“è·¯å¾„: {self.persist_directory}\\n\\n"
            
            # æ¥æºåˆ†å¸ƒ
            stats += f"ğŸ“ **æ¥æºåˆ†å¸ƒ** (Top 10)\\n"
            top_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)[:10]
            for source, count in top_sources:
                percentage = (count / total_docs) * 100
                stats += f"  - {source}: {count} å— ({percentage:.1f}%)\\n"
            if len(sources) > 10:
                stats += f"  ... è¿˜æœ‰ {len(sources) - 10} ä¸ªæ¥æº\\n"
            stats += "\\n"
            
            # æ–‡ä»¶ç±»å‹åˆ†å¸ƒ
            if extensions:
                stats += f"ğŸ“„ **æ–‡ä»¶ç±»å‹åˆ†å¸ƒ**\\n"
                for ext, count in sorted(extensions.items(), key=lambda x: x[1], reverse=True):
                    percentage = (count / total_docs) * 100
                    stats += f"  - {ext or 'æ— æ‰©å±•å'}: {count} å— ({percentage:.1f}%)\\n"
                stats += "\\n"
            
            # ç³»ç»Ÿä¿¡æ¯
            stats += f"âš™ï¸ **ç³»ç»Ÿä¿¡æ¯**\\n"
            stats += f"  - åµŒå…¥æ¨¡å‹: sentence-transformers/all-MiniLM-L6-v2\\n"
            stats += f"  - å‘é‡ç»´åº¦: 384\\n"
            stats += f"  - æ•°æ®åº“ç±»å‹: Chroma\\n"
            
            return stats
            
        except Exception as e:
            return f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼š{e}"
    
    def _get_database_schema(self) -> str:
        """è·å–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯"""
        try:
            schema = f"ğŸ—„ï¸ **å‘é‡æ•°æ®åº“æ¨¡å¼ä¿¡æ¯**\\n\\n"
            
            schema += f"**åŸºç¡€é…ç½®**\\n"
            schema += f"  - æ•°æ®åº“ç±»å‹: ChromaDB\\n"
            schema += f"  - æŒä¹…åŒ–ç›®å½•: {self.persist_directory}\\n"
            schema += f"  - åµŒå…¥å‡½æ•°: HuggingFaceEmbeddings\\n"
            schema += f"  - æ¨¡å‹åç§°: sentence-transformers/all-MiniLM-L6-v2\\n"
            schema += f"  - å‘é‡ç»´åº¦: 384\\n\\n"
            
            schema += f"**æ•°æ®ç»“æ„**\\n"
            schema += f"  - æ–‡æ¡£å†…å®¹: page_content (text)\\n"
            schema += f"  - å‘é‡åµŒå…¥: embeddings (float[384])\\n"
            schema += f"  - å…ƒæ•°æ®å­—æ®µ:\\n"
            schema += f"    â€¢ source: æ–‡æ¡£æ¥æº\\n"
            schema += f"    â€¢ filename: æ–‡ä»¶å\\n"
            schema += f"    â€¢ file_extension: æ–‡ä»¶æ‰©å±•å\\n"
            schema += f"    â€¢ file_size: æ–‡ä»¶å¤§å°\\n"
            schema += f"    â€¢ chunk_index: å—ç´¢å¼•\\n"
            schema += f"    â€¢ total_chunks: æ€»å—æ•°\\n"
            schema += f"    â€¢ chunk_size: å—å¤§å°\\n"
            schema += f"    â€¢ è‡ªå®šä¹‰å…ƒæ•°æ®...\\n\\n"
            
            schema += f"**æ”¯æŒçš„æ“ä½œ**\\n"
            schema += f"  - similarity_search: å‘é‡ç›¸ä¼¼åº¦æœç´¢\\n"
            schema += f"  - similarity_search_with_score: å¸¦åˆ†æ•°çš„æœç´¢\\n"
            schema += f"  - add_documents: æ·»åŠ æ–‡æ¡£\\n"
            schema += f"  - delete: åˆ é™¤æ–‡æ¡£\\n"
            schema += f"  - get: è·å–æ–‡æ¡£\\n"
            schema += f"  - persist: æŒä¹…åŒ–æ•°æ®\\n"
            
            return schema
            
        except Exception as e:
            return f"è·å–æ•°æ®åº“æ¨¡å¼å¤±è´¥ï¼š{e}"
    
    async def run(self):
        """è¿è¡ŒMCPæœåŠ¡å™¨"""
        print("\\nğŸ¯ å¯åŠ¨RAG MCPæœåŠ¡å™¨...")
        print("ğŸ“¡ ç­‰å¾…å®¢æˆ·ç«¯è¿æ¥...")
        
        async with stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                InitializationOptions()
            )

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RAGç³»ç»ŸMCPæœåŠ¡å™¨")
    print("=" * 50)
    print("ğŸ“š åŠŸèƒ½: æ–‡æ¡£ç®¡ç†ã€å‘é‡æ£€ç´¢ã€æ™ºèƒ½é—®ç­”")
    print("ğŸ”§ æŠ€æœ¯æ ˆ: LangChain + Chroma + OpenAI")
    print("=" * 50)
    
    # åˆ›å»ºå¹¶è¿è¡ŒæœåŠ¡å™¨
    server = RAGMCPServer()
    await server.run()

if __name__ == "__main__":
    asyncio.run(main())