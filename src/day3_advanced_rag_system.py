#!/usr/bin/env python3
"""
ç¬¬3å¤©ï¼šé«˜çº§RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿ
é›†æˆå‘é‡æ•°æ®åº“ã€æ··åˆæœç´¢ã€æŸ¥è¯¢ä¼˜åŒ–ç­‰åŠŸèƒ½
"""

import os
import time
import shutil
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv

# LangChainç›¸å…³å¯¼å…¥
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# ç¯å¢ƒé…ç½®
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç±»"""
    content: str
    metadata: Dict[str, Any]
    chunk_id: str


class RAGResponse(BaseModel):
    """RAGç³»ç»Ÿå“åº”æ¨¡å‹"""
    answer: str = Field(description="åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆçš„ç­”æ¡ˆ")
    confidence: float = Field(description="ç­”æ¡ˆç½®ä¿¡åº¦ (0-1)", ge=0, le=1)
    sources: List[str] = Field(description="å‚è€ƒæ–‡æ¡£æ¥æºåˆ—è¡¨")
    retrieved_chunks: int = Field(description="æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡")
    search_method: str = Field(description="ä½¿ç”¨çš„æœç´¢æ–¹æ³•")
    has_sufficient_context: bool = Field(description="æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯")


class AdvancedRAGSystem:
    """é«˜çº§RAGç³»ç»Ÿå®ç°"""
    
    def __init__(
        self,
        model_name: str = "gpt-3.5-turbo",
        embedding_model: str = "text-embedding-ada-002",
        chunk_size: int = 300,
        chunk_overlap: int = 50,
        vector_db_type: str = "chroma"
    ):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        å‚æ•°:
            model_name: ä½¿ç”¨çš„LLMæ¨¡å‹
            embedding_model: åµŒå…¥æ¨¡å‹
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            vector_db_type: å‘é‡æ•°æ®åº“ç±»å‹ ("chroma" æˆ– "faiss")
        """
        self.llm = ChatOpenAI(model=model_name, temperature=0)
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.vector_db_type = vector_db_type
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\\n\\n", "\\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", ",", " ", ""]
        )
        
        # å‘é‡å­˜å‚¨
        self.vectorstore = None
        self.bm25_retriever = None
        self.hybrid_retriever = None
        
        # æç¤ºè¯æ¨¡æ¿
        self._setup_prompts()
        
        print(f"ğŸ¤– RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   LLM: {model_name}")
        print(f"   å‘é‡æ•°æ®åº“: {vector_db_type}")
        print(f"   åˆ†å—å¤§å°: {chunk_size}")
    
    def _setup_prompts(self):
        """è®¾ç½®æç¤ºè¯æ¨¡æ¿"""
        
        # åŸºç¡€RAGæç¤ºè¯
        self.basic_rag_prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:
        {context}
        
        ç”¨æˆ·é—®é¢˜: {question}
        
        è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
        1. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
        2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
        3. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œå®¢è§‚æ€§
        4. å¯ä»¥é€‚å½“æ•´åˆå¤šä¸ªæ–‡æ¡£çš„ä¿¡æ¯
        
        å›ç­”:
        """)
        
        # é«˜çº§RAGæç¤ºè¯ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰
        self.advanced_rag_prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶æä¾›è¯¦ç»†çš„åˆ†æã€‚
        
        æ£€ç´¢æ–‡æ¡£:
        {context}
        
        ç”¨æˆ·é—®é¢˜: {question}
        
        è¯·åˆ†ææ–‡æ¡£çš„ç›¸å…³æ€§å’Œä¿¡æ¯å……åˆ†æ€§ï¼Œå¹¶è¯„ä¼°ç­”æ¡ˆçš„ç½®ä¿¡åº¦ã€‚
        
        {format_instructions}
        """)
        
        # æŸ¥è¯¢ä¼˜åŒ–æç¤ºè¯
        self.query_optimization_prompt = ChatPromptTemplate.from_template("""
        ä½œä¸ºæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·å°†ç”¨æˆ·æŸ¥è¯¢è½¬æ¢ä¸ºæ›´æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚
        
        åŸå§‹æŸ¥è¯¢: {query}
        
        è¯·ç”Ÿæˆ3ä¸ªä¼˜åŒ–ç‰ˆæœ¬ï¼š
        1. æ·»åŠ ç›¸å…³æŠ€æœ¯æœ¯è¯­çš„å…·ä½“åŒ–æŸ¥è¯¢
        2. ä½¿ç”¨åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µçš„æ‰©å±•æŸ¥è¯¢  
        3. åˆ†è§£ä¸ºå­é—®é¢˜çš„ç»“æ„åŒ–æŸ¥è¯¢
        
        è¾“å‡ºæ ¼å¼ï¼š
        å…·ä½“åŒ–: [æŸ¥è¯¢1]
        æ‰©å±•: [æŸ¥è¯¢2]
        ç»“æ„åŒ–: [æŸ¥è¯¢3]
        """)
    
    def load_documents(self, documents: List[Dict[str, Any]]) -> List[Document]:
        """
        åŠ è½½å’Œå¤„ç†æ–‡æ¡£
        
        å‚æ•°:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«contentå’Œmetadata
        è¿”å›:
            å¤„ç†åçš„Documentå¯¹è±¡åˆ—è¡¨
        """
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£...")
        
        doc_objects = []
        for i, doc_data in enumerate(documents):
            doc = Document(
                page_content=doc_data["content"].strip(),
                metadata={
                    **doc_data.get("metadata", {}),
                    "doc_id": i,
                    "source": doc_data.get("title", f"Document_{i}")
                }
            )
            doc_objects.append(doc)
        
        # æ–‡æ¡£åˆ†å—
        splits = self.text_splitter.split_documents(doc_objects)
        print(f"ğŸ“„ æ–‡æ¡£åˆ†å—å®Œæˆ: {len(doc_objects)} -> {len(splits)} å—")
        
        return splits
    
    def build_vector_database(self, documents: List[Document], persist_path: str = None):
        """
        æ„å»ºå‘é‡æ•°æ®åº“
        
        å‚æ•°:
            documents: æ–‡æ¡£åˆ—è¡¨
            persist_path: æŒä¹…åŒ–è·¯å¾„
        """
        print(f"ğŸ”¨ æ­£åœ¨æ„å»º{self.vector_db_type.upper()}å‘é‡æ•°æ®åº“...")
        
        if self.vector_db_type == "chroma":
            if persist_path and os.path.exists(persist_path):
                shutil.rmtree(persist_path)
            
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=persist_path,
                collection_name="rag_collection"
            )
            
        elif self.vector_db_type == "faiss":
            self.vectorstore = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings
            )
            
            if persist_path:
                self.vectorstore.save_local(persist_path)
        
        # åˆ›å»ºBM25æ£€ç´¢å™¨ï¼ˆå…³é”®è¯æœç´¢ï¼‰
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = 5
        
        # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
        semantic_retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})
        self.hybrid_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, semantic_retriever],
            weights=[0.3, 0.7]  # BM25: 30%, è¯­ä¹‰æœç´¢: 70%
        )
        
        print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£å—")
    
    def optimize_query(self, query: str) -> List[str]:
        """
        æŸ¥è¯¢ä¼˜åŒ–ï¼šç”Ÿæˆå¤šä¸ªæœç´¢å˜ä½“
        
        å‚æ•°:
            query: åŸå§‹æŸ¥è¯¢
        è¿”å›:
            ä¼˜åŒ–åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            chain = self.query_optimization_prompt | self.llm | StrOutputParser()
            result = chain.invoke({"query": query})
            
            # è§£æç»“æœ
            optimized_queries = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
            lines = result.strip().split('\\n')
            
            for line in lines:
                if ':' in line:
                    optimized_query = line.split(':', 1)[1].strip()
                    if optimized_query and optimized_query not in optimized_queries:
                        optimized_queries.append(optimized_query)
            
            return optimized_queries[:4]  # æœ€å¤šè¿”å›4ä¸ªæŸ¥è¯¢
            
        except Exception as e:
            print(f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
            return [query]
    
    def retrieve_documents(
        self, 
        query: str, 
        method: str = "hybrid",
        k: int = 5,
        use_query_optimization: bool = False
    ) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            method: æ£€ç´¢æ–¹æ³• ("semantic", "bm25", "hybrid")
            k: è¿”å›æ–‡æ¡£æ•°é‡
            use_query_optimization: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢ä¼˜åŒ–
        è¿”å›:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.vectorstore:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨build_vector_database")
        
        queries = [query]
        if use_query_optimization:
            queries = self.optimize_query(query)
            print(f"ğŸ” ä½¿ç”¨ä¼˜åŒ–æŸ¥è¯¢: {len(queries)} ä¸ªå˜ä½“")
        
        # æ”¶é›†æ‰€æœ‰æ£€ç´¢ç»“æœ
        all_docs = []
        doc_scores = {}
        
        for q in queries:
            if method == "semantic":
                docs = self.vectorstore.similarity_search(q, k=k)
            elif method == "bm25":
                docs = self.bm25_retriever.get_relevant_documents(q)[:k]
            elif method == "hybrid":
                docs = self.hybrid_retriever.get_relevant_documents(q)[:k]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢æ–¹æ³•: {method}")
            
            # è®¡ç®—æ–‡æ¡£å¾—åˆ†ï¼ˆåŸºäºå‡ºç°é¢‘æ¬¡ï¼‰
            for doc in docs:
                doc_id = doc.metadata.get('source', 'unknown')
                if doc_id in doc_scores:
                    doc_scores[doc_id] += 1
                else:
                    doc_scores[doc_id] = 1
                    all_docs.append(doc)
        
        # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›top-k
        sorted_docs = sorted(
            all_docs, 
            key=lambda x: doc_scores[x.metadata.get('source', 'unknown')], 
            reverse=True
        )
        
        return sorted_docs[:k]
    
    def format_context(self, documents: List[Document]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', f'æ–‡æ¡£{i}')
            content = doc.page_content.strip()
            context_parts.append(f"[æ–‡æ¡£{i}: {source}]\\n{content}")
        
        return "\\n\\n".join(context_parts)
    
    def generate_answer(
        self, 
        query: str, 
        method: str = "hybrid",
        use_advanced: bool = False,
        use_query_optimization: bool = False
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆRAGç­”æ¡ˆ
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            method: æ£€ç´¢æ–¹æ³•
            use_advanced: æ˜¯å¦ä½¿ç”¨é«˜çº§æ¨¡å¼ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰
            use_query_optimization: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢ä¼˜åŒ–
        è¿”å›:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        
        # æ£€ç´¢æ–‡æ¡£
        documents = self.retrieve_documents(
            query, 
            method=method, 
            k=5,
            use_query_optimization=use_query_optimization
        )
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self.format_context(documents)
        
        # ç”Ÿæˆç­”æ¡ˆ
        if use_advanced:
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            parser = PydanticOutputParser(pydantic_object=RAGResponse)
            
            chain = (
                self.advanced_rag_prompt | 
                self.llm | 
                parser
            )
            
            try:
                response = chain.invoke({
                    "context": context,
                    "question": query,
                    "format_instructions": parser.get_format_instructions()
                })
                
                result = {
                    "answer": response.answer,
                    "confidence": response.confidence,
                    "sources": response.sources,
                    "retrieved_chunks": len(documents),
                    "search_method": method,
                    "has_sufficient_context": response.has_sufficient_context,
                    "processing_time": time.time() - start_time,
                    "retrieved_documents": documents
                }
                
            except Exception as e:
                print(f"ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥: {e}")
                # å›é€€åˆ°åŸºç¡€æ¨¡å¼
                use_advanced = False
        
        if not use_advanced:
            # åŸºç¡€æ¨¡å¼
            chain = (
                self.basic_rag_prompt |
                self.llm |
                StrOutputParser()
            )
            
            answer = chain.invoke({
                "context": context,
                "question": query
            })
            
            result = {
                "answer": answer,
                "retrieved_chunks": len(documents),
                "search_method": method,
                "processing_time": time.time() - start_time,
                "retrieved_documents": documents,
                "sources": [doc.metadata.get('source', 'Unknown') for doc in documents]
            }
        
        return result
    
    def evaluate_retrieval(self, test_cases: List[Dict]) -> Dict[str, float]:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½
        
        å‚æ•°:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ŒåŒ…å«queryå’Œexpected_sources
        è¿”å›:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print("ğŸ“Š å¼€å§‹æ£€ç´¢æ€§èƒ½è¯„ä¼°...")
        
        methods = ["semantic", "bm25", "hybrid"]
        results = {method: {"precision": 0, "recall": 0, "f1": 0} for method in methods}
        
        for method in methods:
            precision_scores = []
            recall_scores = []
            
            for test_case in test_cases:
                query = test_case["query"]
                expected_sources = set(test_case["expected_sources"])
                
                # æ£€ç´¢æ–‡æ¡£
                retrieved_docs = self.retrieve_documents(query, method=method, k=5)
                retrieved_sources = set([doc.metadata.get('source', '') for doc in retrieved_docs])
                
                # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
                if retrieved_sources:
                    precision = len(expected_sources & retrieved_sources) / len(retrieved_sources)
                    precision_scores.append(precision)
                
                if expected_sources:
                    recall = len(expected_sources & retrieved_sources) / len(expected_sources)
                    recall_scores.append(recall)
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0
            avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0
            f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0
            
            results[method] = {
                "precision": avg_precision,
                "recall": avg_recall,
                "f1": f1_score
            }
            
            print(f"{method.upper()} - P: {avg_precision:.3f}, R: {avg_recall:.3f}, F1: {f1_score:.3f}")
        
        return results


def create_sample_knowledge_base():
    """åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“"""
    return [
        {
            "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
            "content": """
            äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨å’Œè½¯ä»¶ã€‚
            AIçš„ä¸»è¦ç›®æ ‡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’Œé—®é¢˜è§£å†³ã€‚ç°ä»£AIä¸»è¦åŸºäºæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ã€‚
            AIåº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚
            """,
            "metadata": {"category": "åŸºç¡€æ¦‚å¿µ", "keywords": ["äººå·¥æ™ºèƒ½", "AI", "æœºå™¨å­¦ä¹ "]}
        },
        {
            "title": "æ·±åº¦å­¦ä¹ æŠ€æœ¯",
            "content": """
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡å’Œç†è§£å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚
            å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸»è¦ç”¨äºå›¾åƒå¤„ç†ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚
            å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰é€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚
            Transformeræ¶æ„å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒGPTã€BERTç­‰å¤§å‹è¯­è¨€æ¨¡å‹éƒ½åŸºäºæ­¤æ¶æ„ã€‚
            """,
            "metadata": {"category": "æŠ€æœ¯è¯¦è§£", "keywords": ["æ·±åº¦å­¦ä¹ ", "CNN", "RNN", "Transformer"]}
        },
        {
            "title": "è‡ªç„¶è¯­è¨€å¤„ç†",
            "content": """
            è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„é‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›äºè®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
            ä¼ ç»ŸNLPæŠ€æœ¯åŒ…æ‹¬åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ã€å¥æ³•åˆ†æç­‰åŸºç¡€ä»»åŠ¡ã€‚
            ç°ä»£NLPä¸»è¦åŸºäºæ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯Transformeræ¶æ„çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
            ä¸»è¦åº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚
            """,
            "metadata": {"category": "åº”ç”¨é¢†åŸŸ", "keywords": ["NLP", "è‡ªç„¶è¯­è¨€å¤„ç†", "æœºå™¨ç¿»è¯‘"]}
        }
    ]


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºRAGç³»ç»ŸåŠŸèƒ½"""
    print("ğŸš€ é«˜çº§RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = AdvancedRAGSystem(
        chunk_size=200,
        chunk_overlap=30,
        vector_db_type="chroma"
    )
    
    # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
    documents_data = create_sample_knowledge_base()
    documents = rag_system.load_documents(documents_data)
    
    # æ„å»ºå‘é‡æ•°æ®åº“
    rag_system.build_vector_database(documents, persist_path="./rag_chroma_db")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "CNNåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨",
        "Transformeræ¶æ„çš„ç‰¹ç‚¹",
        "NLPæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ"
    ]
    
    print("\\nğŸ§ª RAGç³»ç»ŸåŠŸèƒ½æµ‹è¯•")
    print("-" * 30)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\\n--- æµ‹è¯• {i} ---")
        print(f"æŸ¥è¯¢: {query}")
        
        # æµ‹è¯•ä¸åŒæ£€ç´¢æ–¹æ³•
        for method in ["semantic", "hybrid"]:
            print(f"\\nğŸ“ {method.upper()} æ–¹æ³•:")
            
            result = rag_system.generate_answer(
                query, 
                method=method,
                use_advanced=True,
                use_query_optimization=(method == "hybrid")
            )
            
            print(f"ç­”æ¡ˆ: {result['answer'][:200]}...")
            if 'confidence' in result:
                print(f"ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f}")
            print(f"æ£€ç´¢æ–‡æ¡£æ•°: {result['retrieved_chunks']}")
            print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.2f}s")
            print(f"å‚è€ƒæ¥æº: {', '.join(result['sources'][:3])}")
    
    # ç®€å•çš„äº¤äº’å¼æ¼”ç¤º
    print("\\nğŸ’¬ äº¤äº’å¼é—®ç­” (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 30)
    
    while True:
        try:
            user_query = input("\\nä½ çš„é—®é¢˜: ").strip()
            if user_query.lower() == 'quit':
                break
            
            if user_query:
                result = rag_system.generate_answer(
                    user_query, 
                    method="hybrid",
                    use_advanced=False,
                    use_query_optimization=True
                )
                
                print(f"\\nğŸ¤– å›ç­”: {result['answer']}")
                print(f"ğŸ“š å‚è€ƒ: {', '.join(result['sources'][:2])}")
        
        except KeyboardInterrupt:
            break
    
    print("\\nğŸ‘‹ æ¼”ç¤ºç»“æŸ")


if __name__ == "__main__":
    main()