{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2天：LangChain框架深入学习\n",
    "\n",
    "## 今日学习目标\n",
    "1. 掌握LangChain的核心架构\n",
    "2. 深入理解Prompt Templates和Output Parsers\n",
    "3. 实现多种Memory系统\n",
    "4. 学习Document处理流程\n",
    "5. 构建一个完整的对话系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 环境配置完成\n"
     ]
    }
   ],
   "source": [
    "# 环境设置\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "print(\"✅ 环境配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChain核心架构理解\n",
    "\n",
    "### LangChain的6大核心组件\n",
    "1. **Models（模型）**：各种LLM的统一接口\n",
    "2. **Prompts（提示词）**：管理和优化提示词\n",
    "3. **Chains（链）**：组合多个组件的工作流\n",
    "4. **Memory（记忆）**：维持对话状态和历史\n",
    "5. **Agents（代理）**：让LLM选择和执行工具\n",
    "6. **Callbacks（回调）**：监控和调试执行过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 LLM模型初始化完成\n"
     ]
    }
   ],
   "source": [
    "# 核心导入\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "from langchain.schema import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "# 初始化基础模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"🤖 LLM模型初始化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates深入实践\n",
    "\n",
    "### 2.1 基础Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 基础Prompt Template测试 ===\n",
      "\n",
      "--- 测试 1 ---\n",
      "角色: AI技术专家\n",
      "回答: Transformer是一种深度学习模型架构，最早在2017年被Google提出，专门用来处理序列数据，比如自然语言（文本）。它革新了自然语言处理（NLP）领域，现在很多强大的AI模型（比如ChatGPT、BERT）都是基于Transformer的。\n",
      "\n",
      "用通俗的话来说，Transformer就像是一个超级“翻译官”，能够理解和转换长短不一的文本、语音甚至代码。相比传统的循环神经网络（RNN）和卷积...\n",
      "\n",
      "--- 测试 2 ---\n",
      "角色: 诗人\n",
      "回答: 春天是一首温柔的诗，  \n",
      "在细雨和晨曦中慢慢展开。  \n",
      "嫩绿的芽尖，悄悄探出泥土，  \n",
      "像初醒的梦想，微微颤抖。  \n",
      "桃花粉红，柳絮轻扬，  \n",
      "风，带着泥土与花香，  \n",
      "唤醒沉睡一冬的河流和鸟鸣。  \n",
      "田野在阳光下渐渐铺开，  \n",
      "小草偷偷地装点着大地的衣裳。  \n",
      "一切都是崭新的，温柔的，  \n",
      "仿佛每一寸空气里都藏着希望，  \n",
      "在这个季节，连呼吸都是诗行。...\n"
     ]
    }
   ],
   "source": [
    "# 基础提示词模板\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个专业的{role}，擅长{skill}。请用{style}的风格回答问题。\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 创建基础链\n",
    "basic_chain = basic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 测试不同角色\n",
    "print(\"=== 基础Prompt Template测试 ===\")\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"role\": \"AI技术专家\",\n",
    "        \"skill\": \"深度学习和机器学习\",\n",
    "        \"style\": \"通俗易懂\",\n",
    "        \"question\": \"什么是Transformer架构？\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"诗人\",\n",
    "        \"skill\": \"现代诗创作\",\n",
    "        \"style\": \"优美抒情\",\n",
    "        \"question\": \"描述一下春天的景色\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n--- 测试 {i} ---\")\n",
    "    response = basic_chain.invoke(case)\n",
    "    print(f\"角色: {case['role']}\")\n",
    "    print(f\"回答: {response[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 条件化Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 条件化Prompt Template测试 ===\n",
      "\n",
      "--- BEGINNER 版本 ---\n",
      "回答: 好的，让我们用最简单的方式来理解RESTful API。\n",
      "\n",
      "**1. API是什么？**  \n",
      "API 就像一个餐厅的菜单。你（客户端，比如手机App或网页）想点菜（获取或发送数据），但你不能直接进厨房（服务器）。你通过菜单（API）告诉服务员（程序）你要什么，服务员帮你把菜从厨房带出来。\n",
      "\n",
      "**2. RESTful是什么意思？**  \n",
      "RESTful 其实是一种设计API的方法和风格。REST 代表“表现层状态转移（Representational State Transfer）”，名字听起来有点复杂，但意思很简单：  \n",
      "- 它用“网址”（URL）来指代资源（比如用户、商品、文章等）。\n",
      "- 它...\n",
      "\n",
      "--- INTERMEDIATE 版本 ---\n",
      "回答: RESTful API（Representational State Transfer，表现层状态转移的应用程序接口）是一种基于HTTP协议、遵循REST架构风格的Web服务设计方式。它允许不同的系统通过标准的HTTP动词（如GET、POST、PUT、DELETE等）进行通信，常见于Web开发中前后端的数据交互。\n",
      "\n",
      "### 1. 基本原理\n",
      "\n",
      "REST强调资源(Resource)的概念，每个资源由一个URL唯一标识。客户端通过HTTP方法对资源进行操作：\n",
      "\n",
      "| HTTP方法 | 作用             | 说明           |\n",
      "|----------|---------------...\n",
      "\n",
      "--- EXPERT 版本 ---\n",
      "回答: RESTful API（Representational State Transfer Application Programming Interface）是一种基于REST架构风格设计的Web API。REST由Roy Fielding在2000年提出，是一种轻量级、无状态、以资源为中心的分布式系统架构。RESTful API广泛用于Web服务的数据交互，具有良好的可扩展性、易用性和与HTTP协议的高度契合。\n",
      "\n",
      "## 核心概念\n",
      "\n",
      "1. **资源（Resource）**\n",
      "   - 一切皆资源。资源通常用URI（统一资源标识符）唯一标识，如`/users/123`。\n",
      "2. **HTTP方法（Me...\n"
     ]
    }
   ],
   "source": [
    "# 条件化提示词（根据用户类型调整）\n",
    "def create_conditional_prompt(user_type: str) -> ChatPromptTemplate:\n",
    "    \"\"\"根据用户类型创建不同的提示词\"\"\"\n",
    "    \n",
    "    if user_type == \"beginner\":\n",
    "        system_message = \"\"\"\n",
    "        你是一个耐心的AI导师，专门为初学者解答问题。\n",
    "        请用简单易懂的语言解释，避免使用专业术语。\n",
    "        如果必须使用术语，请提供简单的解释。\n",
    "        用类比和例子帮助理解。\n",
    "        \"\"\"\n",
    "    elif user_type == \"expert\":\n",
    "        system_message = \"\"\"\n",
    "        你是一个技术专家，为有经验的开发者提供深入分析。\n",
    "        可以使用专业术语和技术细节。\n",
    "        提供代码示例和最佳实践。\n",
    "        关注性能优化和架构设计。\n",
    "        \"\"\"\n",
    "    else:  # intermediate\n",
    "        system_message = \"\"\"\n",
    "        你是一个经验丰富的技术顾问。\n",
    "        提供中等深度的技术解释。\n",
    "        包含一些代码示例和实践建议。\n",
    "        平衡理论和实践应用。\n",
    "        \"\"\"\n",
    "    \n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_message.strip()),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "# 测试条件化提示词\n",
    "print(\"=== 条件化Prompt Template测试 ===\")\n",
    "question = \"什么是RESTful API？\"\n",
    "\n",
    "for user_type in [\"beginner\", \"intermediate\", \"expert\"]:\n",
    "    print(f\"\\n--- {user_type.upper()} 版本 ---\")\n",
    "    prompt = create_conditional_prompt(user_type)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(f\"回答: {response[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output Parsers：结构化输出\n",
    "\n",
    "### 3.1 JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON Output Parser测试 ===\n",
      "\n",
      "--- 分析：Docker容器化 ---\n",
      "概念: Docker容器化\n",
      "定义: Docker容器化是一种利用Docker平台将应用及其依赖打包在独立、可移植容器中的技术，实现环境一致性和高效部署。\n",
      "难度: medium\n",
      "学习时间: 1-2周（基础到入门应用）\n",
      "优点: 应用部署和迁移速度快, 保证开发、测试、生产环境一致性, 资源利用率高，便于水平扩展, 易于微服务架构实现\n",
      "\n",
      "--- 分析：GraphQL ---\n",
      "概念: GraphQL\n",
      "定义: GraphQL是一种用于API的查询语言以及相应的服务器端运行时，用于根据客户端需求返回数据。\n",
      "难度: medium\n",
      "学习时间: 1-2周（熟悉基本概念、语法和典型应用）\n",
      "优点: 客户端可以精确获取所需数据，减少冗余传输, 单一端点统一管理多种资源, 强类型系统，易于前后端协作和文档生成, 高效实现聚合查询，减少多次请求\n"
     ]
    }
   ],
   "source": [
    "# JSON输出解析器\n",
    "json_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    你是一个技术分析师。请分析给定的技术概念，并以JSON格式返回结果。\n",
    "    \n",
    "    返回格式：\n",
    "    {{\n",
    "        \"concept\": \"技术概念名称\",\n",
    "        \"definition\": \"简短定义\",\n",
    "        \"pros\": [\"优点1\", \"优点2\"],\n",
    "        \"cons\": [\"缺点1\", \"缺点2\"],\n",
    "        \"use_cases\": [\"使用场景1\", \"使用场景2\"],\n",
    "        \"difficulty\": \"easy/medium/hard\",\n",
    "        \"learning_time\": \"预估学习时间\"\n",
    "    }}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"请分析：{concept}\")\n",
    "])\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_chain = json_prompt | llm | json_parser\n",
    "\n",
    "# 测试JSON解析\n",
    "print(\"=== JSON Output Parser测试 ===\")\n",
    "concepts = [\"Docker容器化\", \"GraphQL\"]\n",
    "\n",
    "for concept in concepts:\n",
    "    print(f\"\\n--- 分析：{concept} ---\")\n",
    "    try:\n",
    "        result = json_chain.invoke({\"concept\": concept})\n",
    "        print(f\"概念: {result['concept']}\")\n",
    "        print(f\"定义: {result['definition']}\")\n",
    "        print(f\"难度: {result['difficulty']}\")\n",
    "        print(f\"学习时间: {result['learning_time']}\")\n",
    "        print(f\"优点: {', '.join(result['pros'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"解析错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pydantic Output Parser（类型安全）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pydantic Output Parser测试 ===\n",
      "📋 分析结果类型: <class '__main__.TechAnalysis'>\n",
      "🎯 概念: 微服务架构\n",
      "📝 定义: 微服务架构是一种将单一应用程序划分为一组小的、独立的服务的方法，每个服务运行在其独立的进程中，并通过轻量级通信机制（通常是HTTP API）相互协作。每个微服务围绕着特定的业务能力构建，能够独立部署和扩展，通常由小型团队负责开发和维护。\n",
      "⭐ 优点: ['易于扩展：每个服务可以独立伸缩，满足不同模块的性能需求。', '技术多样性：不同微服务可以使用最合适的技术栈和编程语言开发。', '高可用性：单个服务故障不会影响整个系统，提升系统容错能力。', '灵活部署：支持持续交付和持续部署，加快产品迭代速度。', '便于团队协作：小团队可独立负责特定微服务，减少沟通成本。']\n",
      "⚠️ 缺点: ['系统复杂性增加：服务数量增多导致部署、运维和监控难度上升。', '分布式系统挑战：需要处理服务间通信、数据一致性、网络延迟等复杂问题。', '运维成本提升：需要引入服务注册、发现、配置管理、链路追踪等基础设施。', '测试困难：集成测试和端到端测试更加复杂，故障定位难度增加。', '数据管理复杂：跨服务的数据一致性和事务管理成为难点。']\n",
      "🎚️ 难度: hard\n",
      "⏰ 学习时间: 1-3个月（有分布式系统基础）\n",
      "🔗 相关技术: ['容器化（Docker、Kubernetes）', '服务网格（Istio、Linkerd）', 'API网关（Kong、Nginx、Zuul）', '服务注册与发现（Consul、Eureka、Zookeeper）', '持续集成/持续部署（CI/CD）', '消息队列（Kafka、RabbitMQ、RocketMQ）', 'Spring Cloud', 'gRPC']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 定义Pydantic模型\n",
    "class TechAnalysis(BaseModel):\n",
    "    \"\"\"技术分析结果模型\"\"\"\n",
    "    concept: str = Field(description=\"技术概念名称\")\n",
    "    definition: str = Field(description=\"技术定义\")\n",
    "    pros: List[str] = Field(description=\"优点列表\")\n",
    "    cons: List[str] = Field(description=\"缺点列表\")\n",
    "    difficulty: str = Field(description=\"难度等级：easy/medium/hard\")\n",
    "    learning_time: str = Field(description=\"预估学习时间\")\n",
    "    related_technologies: List[str] = Field(description=\"相关技术\")\n",
    "\n",
    "# 创建Pydantic解析器\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=TechAnalysis)\n",
    "\n",
    "pydantic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    你是一个技术分析专家。请分析给定的技术概念。\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"请详细分析：{concept}\")\n",
    "])\n",
    "\n",
    "pydantic_chain = pydantic_prompt | llm | pydantic_parser\n",
    "\n",
    "# 测试Pydantic解析\n",
    "print(\"=== Pydantic Output Parser测试 ===\")\n",
    "concept = \"微服务架构\"\n",
    "\n",
    "try:\n",
    "    result = pydantic_chain.invoke({\n",
    "        \"concept\": concept,\n",
    "        \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "    })\n",
    "    \n",
    "    print(f\"📋 分析结果类型: {type(result)}\")\n",
    "    print(f\"🎯 概念: {result.concept}\")\n",
    "    print(f\"📝 定义: {result.definition}\")\n",
    "    print(f\"⭐ 优点: {result.pros}\")\n",
    "    print(f\"⚠️ 缺点: {result.cons}\")\n",
    "    print(f\"🎚️ 难度: {result.difficulty}\")\n",
    "    print(f\"⏰ 学习时间: {result.learning_time}\")\n",
    "    print(f\"🔗 相关技术: {result.related_technologies}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"解析错误: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory系统：让AI记住对话\n",
    "\n",
    "### 4.1 ConversationBufferMemory（完整记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ConversationBufferMemory测试 ===\n",
      "\n",
      "--- 对话轮次 1 ---\n",
      "用户: 我的名字是张三，我是一名Python开发者\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "\n",
      "当前记忆:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "\n",
      "--- 对话轮次 2 ---\n",
      "用户: 我正在学习AI技术，特别是LangChain\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "\n",
      "当前记忆:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "\n",
      "--- 对话轮次 3 ---\n",
      "用户: 你还记得我的名字和职业吗？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "Human: 你还记得我的名字和职业吗？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 当然记得！你的名字是张三，你是一名Python开发者。你最近正在学习AI技术，特别是关注LangChain。如果有任何关于Python或LangChain相关的问题，随时可以问我哦！\n",
      "\n",
      "当前记忆:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "Human: 你还记得我的名字和职业吗？\n",
      "AI: 当然记得！你的名字是张三，你是一名Python开发者。你最近正在学习AI技术，特别是关注LangChain。如果有任何关于Python或LangChain相关的问题，随时可以问我哦！\n",
      "\n",
      "--- 对话轮次 4 ---\n",
      "用户: 我之前提到我在学习什么技术？\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "Human: 你还记得我的名字和职业吗？\n",
      "AI: 当然记得！你的名字是张三，你是一名Python开发者。你最近正在学习AI技术，特别是关注LangChain。如果有任何关于Python或LangChain相关的问题，随时可以问我哦！\n",
      "Human: 我之前提到我在学习什么技术？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 你之前提到你正在学习AI技术，特别是LangChain。LangChain 是一个用于构建基于大语言模型（LLM）应用的框架，非常适合与Python结合开发智能问答、文档检索等AI应用。如果你对LangChain的某个具体功能或者实现细节感兴趣，可以随时告诉我！\n",
      "\n",
      "当前记忆:\n",
      "Human: 我的名字是张三，我是一名Python开发者\n",
      "AI: 你好，张三！很高兴认识你。作为一名Python开发者，你平时主要用Python做哪些方面的开发呢？比如Web开发、数据分析、自动化脚本、机器学习，还是其他的方向？如果你有任何技术上的问题，或者想要交流一些Python相关的经验和资源，随时可以告诉我哦！\n",
      "Human: 我正在学习AI技术，特别是LangChain\n",
      "AI: 太棒了！LangChain 目前是非常热门的 AI 技术，尤其在构建基于大语言模型（LLM）的应用方面非常有用。它为开发者提供了一个灵活的框架，可以很方便地集成各种语言模型（比如 OpenAI 的 GPT 系列），并且支持与文档检索、工具调用、多轮对话等多种功能的集成。\n",
      "\n",
      "简单介绍一下 LangChain 的主要特点和应用场景：\n",
      "\n",
      "1. **链式思维（Chains）**：你可以把多个语言模型调用、工具调用等操作组合成“链”，实现复杂的业务逻辑，比如问答系统、智能客服、文档摘要等。\n",
      "\n",
      "2. **工具集成（Tools & Agents）**：LangChain 支持集成搜索引擎、数据库、Python 解释器等工具，还可以让模型主动选择和调用这些工具来完成任务。\n",
      "\n",
      "3. **文档问答（Document QA）**：通过与 Embedding 模型结合，可以让你的应用在本地文档、知识库中检索答案，常用的有 FAISS、ChromaDB 等向量数据库。\n",
      "\n",
      "4. **多模态和插件扩展**：支持文本、图片等多种输入类型，插件生态也很丰富，比如与 Streamlit、FastAPI 等 Web 框架结合。\n",
      "\n",
      "如果你刚刚开始学 LangChain，推荐先从它的官方文档和 Github 仓库入手，里面有很多实用的教程和代码示例（[LangChain 官方文档](https://python.langchain.com/)）。另外，LangChain 社区也很活跃，如果遇到问题可以在 GitHub Issues、Discussions 里提问。\n",
      "\n",
      "如果你有具体想实现的功能、遇到的技术难题，或者想了解一些最佳实践，随时告诉我，我可以提供更详细的建议和代码示例！你目前对 LangChain 的哪一块最感兴趣呢？\n",
      "Human: 你还记得我的名字和职业吗？\n",
      "AI: 当然记得！你的名字是张三，你是一名Python开发者。你最近正在学习AI技术，特别是关注LangChain。如果有任何关于Python或LangChain相关的问题，随时可以问我哦！\n",
      "Human: 我之前提到我在学习什么技术？\n",
      "AI: 你之前提到你正在学习AI技术，特别是LangChain。LangChain 是一个用于构建基于大语言模型（LLM）应用的框架，非常适合与Python结合开发智能问答、文档检索等AI应用。如果你对LangChain的某个具体功能或者实现细节感兴趣，可以随时告诉我！\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 创建完整记忆系统\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== ConversationBufferMemory测试 ===\")\n",
    "\n",
    "# 模拟对话\n",
    "conversation_topics = [\n",
    "    \"我的名字是张三，我是一名Python开发者\",\n",
    "    \"我正在学习AI技术，特别是LangChain\",\n",
    "    \"你还记得我的名字和职业吗？\",\n",
    "    \"我之前提到我在学习什么技术？\"\n",
    "]\n",
    "\n",
    "for i, topic in enumerate(conversation_topics, 1):\n",
    "    print(f\"\\n--- 对话轮次 {i} ---\")\n",
    "    print(f\"用户: {topic}\")\n",
    "    response = conversation.predict(input=topic)\n",
    "    print(f\"AI: {response}\")\n",
    "    \n",
    "    # 显示记忆内容\n",
    "    print(f\"\\n当前记忆:\")\n",
    "    print(buffer_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ConversationBufferWindowMemory（滑动窗口记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ConversationBufferWindowMemory测试 ===\n",
      "只记住最近3轮对话\n",
      "\n",
      "第1轮：我喜欢蓝色\n",
      "AI: 蓝色真是一个很美的颜色！它常常让人联想到天空和大海，给人一种宁静、清新的感觉。你喜欢蓝色的哪一种呢？比如说天蓝、宝石蓝、藏青色，还是其他什么蓝色？你有没有喜欢的蓝色物品，比如衣服、文具，或者房间的装饰...\n",
      "记忆轮次: 1\n",
      "\n",
      "第2轮：我的爱好是编程\n",
      "AI: 编程真棒！它不仅能锻炼逻辑思维，还能让你创造出各种有趣的项目。你是怎么开始接触编程的呢？你平时用什么编程语言比较多？比如 Python、Java、C++ 还是 JavaScript？你有没有做过一些让...\n",
      "记忆轮次: 2\n",
      "\n",
      "第3轮：我住在北京\n",
      "AI: 北京是一座充满历史与现代气息的城市！这里既有壮丽的古迹，比如故宫、天坛、长城，也有繁华的都市生活，比如三里屯、国贸、798艺术区。北京的小吃也很有特色，像豆汁儿、炸酱面、卤煮火烧、冰糖葫芦，都让人难忘...\n",
      "记忆轮次: 3\n",
      "\n",
      "第4轮：我今年25岁\n",
      "AI: 25岁正是充满活力和无限可能的年纪！这个阶段，很多人会在学习、工作、生活中不断探索和成长。你现在是在上学、刚开始工作，还是已经有一些职业经验了呢？有没有什么目标或者梦想是你特别想实现的？\n",
      "\n",
      "其实，25...\n",
      "记忆轮次: 4\n",
      "\n",
      "第5轮：我学过Java\n",
      "AI: 太棒了！Java 是一门非常实用而且流行的编程语言，无论是在企业级开发、移动应用（比如 Android）、还是后端开发中都非常常见。你学 Java 有多久了？主要是自学、在学校课程中学的，还是工作中用...\n",
      "记忆轮次: 5\n",
      "\n",
      "第6轮：你还记得我喜欢什么颜色吗？\n",
      "AI: 当然记得！你喜欢蓝色。蓝色是一个很受欢迎的颜色，既有清新的天蓝，也有深邃的宝石蓝和藏青色。之前我们还聊到过关于蓝色有什么不同的色调和你喜欢的蓝色物品。如果你还想聊更多和蓝色相关的话题，比如蓝色的搭配、...\n",
      "记忆轮次: 6\n"
     ]
    }
   ],
   "source": [
    "# 创建滑动窗口记忆（只记住最近3轮对话）\n",
    "window_memory = ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "window_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== ConversationBufferWindowMemory测试 ===\")\n",
    "print(\"只记住最近3轮对话\")\n",
    "\n",
    "# 进行多轮对话测试\n",
    "test_messages = [\n",
    "    \"第1轮：我喜欢蓝色\",\n",
    "    \"第2轮：我的爱好是编程\", \n",
    "    \"第3轮：我住在北京\",\n",
    "    \"第4轮：我今年25岁\",\n",
    "    \"第5轮：我学过Java\",\n",
    "    \"第6轮：你还记得我喜欢什么颜色吗？\"  # 应该不记得，因为超出窗口\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    response = window_conversation.predict(input=msg)\n",
    "    print(f\"\\n{msg}\")\n",
    "    print(f\"AI: {response[:100]}...\")\n",
    "    print(f\"记忆轮次: {len(window_memory.chat_memory.messages)//2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ConversationSummaryMemory（摘要记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18889\\AppData\\Local\\Temp\\ipykernel_31376\\2700894171.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  summary_memory = ConversationSummaryMemory(\n",
      "C:\\Users\\18889\\AppData\\Local\\Temp\\ipykernel_31376\\2700894171.py:15: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  summary_conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ConversationSummaryMemory测试 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "轮次 1: 我是一名AI研究员，专注于自然语言处理\n",
      "AI: 太棒了！很高兴认识你。自然语言处理（NLP）是人工智能领域最活跃、最有前景的研究方向之一。你最近在关注哪些NLP子领域呢？比如大语言模型、对话系统、信息抽取、文本生成，还是多模态学习？如果你愿意分享的话，也很想听听你目前参与的项目或者遇到的有趣挑战。...\n",
      "摘要: A human介绍自己是一名专注于自然语言处理的AI研究员。AI表示很高兴认识对方，并指出自然语言处理是人工智能领域中最活跃、最有前景的方向之一。AI还询问对方最近关注的NLP子领域，以及目前参与的项...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "轮次 2: 我最近在研究大语言模型的微调技术\n",
      "AI: 太棒了！大语言模型的微调（fine-tuning）一直是当前NLP领域的热门话题，也是提升模型在特定任务上表现的重要手段。近年来，随着LLM（如GPT、T5、BERT等）的广泛应用，微调技术也在不断发展和演进。不知道你最近主要关注哪一类微调方法？比如：\n",
      "\n",
      "1. **全参数微调（Full-Parame...\n",
      "摘要: A human介绍自己是一名专注于自然语言处理的AI研究员。AI表示很高兴认识对方，并指出自然语言处理是人工智能领域中最活跃、最有前景的方向之一。AI还询问对方最近关注的NLP子领域，以及目前参与的项...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "轮次 3: 我发现LoRA是一个很有效的参数高效微调方法\n",
      "AI: 完全同意！LoRA（Low-Rank Adaptation）作为一种参数高效微调方法，近年来在大语言模型的应用中非常受欢迎，特别是在参数量巨大的模型（如GPT-3、Llama等）上。LoRA 的核心思想是：在冻结原始模型参数的基础上，仅在部分层中引入低秩可训练的适配器权重。这样大大减少了需要更新和存...\n",
      "摘要: A human介绍自己是一名专注于自然语言处理的AI研究员。AI表示很高兴认识对方，并指出自然语言处理是人工智能领域中最活跃、最有前景的方向之一。AI还询问对方最近关注的NLP子领域，以及目前参与的项...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "轮次 4: 我还尝试过QLoRA，它在内存使用上更优化\n",
      "AI: 是的，QLoRA（Quantized LoRA）确实在内存和计算资源使用方面有了进一步的优化。QLoRA的核心创新在于结合了参数高效微调（如LoRA）和权重量化技术。具体来说，QLoRA通过将基础大语言模型的权重进行低比特（如4-bit）量化，大幅压缩了模型在微调和推理时的内存占用；同时，在量化模型...\n",
      "摘要: A human介绍自己是一名专注于自然语言处理的AI研究员，AI表示很高兴认识对方，并指出NLP是人工智能领域中最活跃、最有前景的方向之一，询问对方当前关注的NLP子领域及项目挑战。Human表示自己...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     24\u001b[39m long_conversation = [\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m我是一名AI研究员，专注于自然语言处理\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m我最近在研究大语言模型的微调技术\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m你能总结一下我们刚才讨论的内容吗？\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m ]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(long_conversation, \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     response = \u001b[43msummary_conversation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m轮次 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[:\u001b[32m150\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\llm.py:319\u001b[39m, in \u001b[36mLLMChain.predict\u001b[39m\u001b[34m(self, callbacks, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    317\u001b[39m \u001b[33;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.output_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:963\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    956\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m     **kwargs: Any,\n\u001b[32m    961\u001b[39m ) -> LLMResult:\n\u001b[32m    962\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:782\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    781\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1028\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1032\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1130\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1085\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1086\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\openai\\_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\http_proxy.py:343\u001b[39m, in \u001b[36mTunnelHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mself\u001b[39m._connection = HTTP11Connection(\n\u001b[32m    337\u001b[39m                 origin=\u001b[38;5;28mself\u001b[39m._remote_origin,\n\u001b[32m    338\u001b[39m                 stream=stream,\n\u001b[32m    339\u001b[39m                 keepalive_expiry=\u001b[38;5;28mself\u001b[39m._keepalive_expiry,\n\u001b[32m    340\u001b[39m             )\n\u001b[32m    342\u001b[39m         \u001b[38;5;28mself\u001b[39m._connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai-guide\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# 创建摘要记忆系统\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=200  # 当记忆超过1000 tokens时开始摘要\n",
    ")\n",
    "\n",
    "summary_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== ConversationSummaryMemory测试 ===\")\n",
    "\n",
    "# 模拟长对话\n",
    "long_conversation = [\n",
    "    \"我是一名AI研究员，专注于自然语言处理\",\n",
    "    \"我最近在研究大语言模型的微调技术\",\n",
    "    \"我发现LoRA是一个很有效的参数高效微调方法\",\n",
    "    \"我还尝试过QLoRA，它在内存使用上更优化\",\n",
    "    \"除了微调，我也在研究RAG技术\",\n",
    "    \"RAG可以让模型获取最新的外部知识\",\n",
    "    \"你能总结一下我们刚才讨论的内容吗？\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(long_conversation, 1):\n",
    "    response = summary_conversation.predict(input=msg)\n",
    "    print(f\"\\n轮次 {i}: {msg}\")\n",
    "    print(f\"AI: {response[:150]}...\")\n",
    "    \n",
    "    # 显示摘要\n",
    "    if hasattr(summary_memory, 'buffer'):\n",
    "        print(f\"摘要: {summary_memory.buffer[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Loaders和Text Splitters\n",
    "\n",
    "### 5.1 处理不同格式的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 文档处理测试 ===\n",
      "原始文档长度: 441 字符\n",
      "原始文档:\n",
      "\n",
      "人工智能（Artificial Intelligence, AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "\n",
      "机器学习（Machine Learning, ML）是人工智能的一个子集，它使计算机能够在没有明确编程的情况下学习和改进。机器学习算法使用统计技术来找到数据中的模式。\n",
      "\n",
      "深度学习（Deep Learning, DL）是机器学习的一个子集，它使用人工神经网络...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter, \n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 创建示例文档\n",
    "sample_text = \"\"\"\n",
    "人工智能（Artificial Intelligence, AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
    "\n",
    "机器学习（Machine Learning, ML）是人工智能的一个子集，它使计算机能够在没有明确编程的情况下学习和改进。机器学习算法使用统计技术来找到数据中的模式。\n",
    "\n",
    "深度学习（Deep Learning, DL）是机器学习的一个子集，它使用人工神经网络来模拟人脑的学习过程。深度学习在图像识别、自然语言处理和语音识别等领域取得了突破性进展。\n",
    "\n",
    "自然语言处理（Natural Language Processing, NLP）是人工智能的一个重要分支，专注于让计算机理解、解释和生成人类语言。NLP结合了计算语言学、机器学习和深度学习技术。\n",
    "\n",
    "大语言模型（Large Language Models, LLMs）如GPT、BERT等，代表了NLP领域的最新进展。这些模型在大量文本数据上进行预训练，能够理解和生成高质量的自然语言文本。\n",
    "\"\"\"\n",
    "\n",
    "# 创建文档对象\n",
    "doc = Document(page_content=sample_text, metadata={\"source\": \"AI技术概述\"})\n",
    "\n",
    "print(\"=== 文档处理测试 ===\")\n",
    "print(f\"原始文档长度: {len(sample_text)} 字符\")\n",
    "print(f\"原始文档:\\n{sample_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 不同的文本分割策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 递归字符分割器 ===\n",
      "分割后块数: 3\n",
      "\n",
      "块 1 (长度: 154)\n",
      "人工智能（Artificial Intelligence, AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "\n",
      "机器学习（Machine Learning, ML）是人工智能的一个子集，它使计算机能够在没有明确编程的情况下学习和改进。机器学习算法使用统计技术来找到数据中...\n",
      "\n",
      "块 2 (长度: 186)\n",
      "深度学习（Deep Learning, DL）是机器学习的一个子集，它使用人工神经网络来模拟人脑的学习过程。深度学习在图像识别、自然语言处理和语音识别等领域取得了突破性进展。\n",
      "\n",
      "自然语言处理（Natural Language Processing, NLP）是人工智能的一个重要分支，专注于让计算机理...\n",
      "\n",
      "块 3 (长度: 95)\n",
      "大语言模型（Large Language Models, LLMs）如GPT、BERT等，代表了NLP领域的最新进展。这些模型在大量文本数据上进行预训练，能够理解和生成高质量的自然语言文本。...\n",
      "\n",
      "=== Token分割器 ===\n",
      "分割后块数: 9\n",
      "\n",
      "块 1:\n",
      "\n",
      "人工智能（Artificial Intelligence, AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "\n",
      "�...\n",
      "\n",
      "块 2:\n",
      "��能的任务的系统。\n",
      "\n",
      "机器学习（Machine Learning, ML）是人工智能的一个子集，它使计算机能够在没有明确编程的情况下学�...\n"
     ]
    }
   ],
   "source": [
    "# 1. 递归字符分割器（推荐）\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # 每块最大字符数\n",
    "    chunk_overlap=50,  # 重叠字符数\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \";\", \",\", \" \", \"\"]  # 分割优先级\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_documents([doc])\n",
    "\n",
    "print(\"\\n=== 递归字符分割器 ===\")\n",
    "print(f\"分割后块数: {len(recursive_chunks)}\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"\\n块 {i+1} (长度: {len(chunk.page_content)})\")\n",
    "    print(f\"{chunk.page_content[:150]}...\")\n",
    "\n",
    "# 2. 基于Token的分割器\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # 每块最大token数\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"\\n=== Token分割器 ===\")\n",
    "print(f\"分割后块数: {len(token_chunks)}\")\n",
    "for i, chunk in enumerate(token_chunks[:2]):  # 只显示前两块\n",
    "    print(f\"\\n块 {i+1}:\")\n",
    "    print(f\"{chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 综合实践：构建智能对话助手\n",
    "\n",
    "将今天学到的所有技术整合到一个完整的对话系统中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18889\\AppData\\Local\\Temp\\ipykernel_31376\\756752796.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferWindowMemory(k=5)  # 记住最近5轮对话\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 智能对话助手测试 ===\n",
      "\n",
      "--- 对话 1 ---\n",
      "用户: 你好，我是小明，我想学习Python编程\n",
      "助手: 你好小明，很高兴能帮助你学习Python编程。你已经有编程经验了吗？还是从零开始学习呢？如果你有任何关于Python编程的问题，都可以随时问我哦！...\n",
      "\n",
      "--- 对话 2 ---\n",
      "用户: 我对机器学习很感兴趣，但不知道从哪里开始\n",
      "助手: 你好！很高兴听到你对机器学习感兴趣。作为一个初学者，你可以从以下几个步骤开始学习机器学习：\n",
      "\n",
      "1. 掌握基础数学知识：机器学习涉及到很多数学概念，特别是线性代数、微积分和概率论等。建议你先打好数学基础。\n",
      "\n",
      "2. 学习编程语言：Python是机器学习领域最常用的编程语言之一，你可以从学习Python开始。掌握Python基础语法和相关库（如NumPy、Pandas、Matplotlib等）对于机器学...\n",
      "\n",
      "--- 对话 3 ---\n",
      "用户: 你还记得我的名字吗？我想学习什么？\n",
      "助手: 当然记得！你是小明，你对机器学习很感兴趣，但不知道从哪里开始学习。如果你想了解如何开始学习机器学习，我可以给你一些建议哦！...\n",
      "\n",
      "--- 对话 4 ---\n",
      "用户: 能给我推荐一些Python机器学习的学习路径吗？\n",
      "助手: 你好！很高兴听到你对Python机器学习感兴趣。作为一个初学者，你可以从以下几个步骤开始学习Python机器学习：\n",
      "\n",
      "1. 掌握基础数学知识：机器学习涉及到很多数学概念，特别是线性代数、微积分和概率论等。建议你先打好数学基础。\n",
      "\n",
      "2. 学习编程语言：Python是机器学习领域最常用的编程语言之一，你可以从学习Python开始。掌握Python基础语法和相关库（如NumPy、Pandas、Matpl...\n",
      "\n",
      "--- 对话 5 ---\n",
      "用户: 谢谢你的建议，我会按照这个路径学习的\n",
      "助手: 不客气，很高兴能帮助到你！如果在学习过程中有任何问题或需要进一步的指导，都可以随时向我咨询。祝你学习顺利，掌握机器学习的知识和技能！如果有其他问题，也欢迎随时和我交流。...\n",
      "\n",
      "=== 对话记忆摘要 ===\n",
      "Human: 你好，我是小明，我想学习Python编程\n",
      "AI: 你好小明，很高兴能帮助你学习Python编程。你已经有编程经验了吗？还是从零开始学习呢？如果你有任何关于Python编程的问题，都可以随时问我哦！\n",
      "Human: 我对机器学习很感兴趣，但不知道从哪里开始\n",
      "AI: 你好！很高兴听到你对机器学习感兴趣。作为一个初学者，你可以从以下几个步骤开始学习机器学习：\n",
      "\n",
      "1. 掌握基础数学知识：机器学习涉及到很多数学概念，特别是线性代数、微积分和概率论等。建议你先打好数学基础。\n",
      "\n",
      "2. 学习编程语言：Python是机器学习领域最常用的编程语言之一，你可以从学习Python开始。掌握Python基础...\n"
     ]
    }
   ],
   "source": [
    "class SmartConversationAssistant:\n",
    "    \"\"\"智能对话助手 - 整合今日所学技术\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "        self.memory = ConversationBufferWindowMemory(k=5)  # 记住最近5轮对话\n",
    "        \n",
    "        # 智能提示词模板\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "            你是一个智能AI助手，具有以下能力：\n",
    "            1. 记住对话历史，提供连贯的回答\n",
    "            2. 根据用户问题类型调整回答风格\n",
    "            3. 提供结构化的信息分析\n",
    "            \n",
    "            对话历史：{history}\n",
    "            \n",
    "            请根据用户的问题类型选择合适的回答方式：\n",
    "            - 技术问题：提供详细解释和代码示例\n",
    "            - 概念解释：用简单易懂的语言和类比\n",
    "            - 个人对话：记住用户信息，提供个性化回答\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # 创建对话链\n",
    "        self.conversation = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=self.prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"进行对话\"\"\"\n",
    "        try:\n",
    "            response = self.conversation.predict(input=message)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"抱歉，处理出错：{str(e)}\"\n",
    "    \n",
    "    def get_memory_summary(self) -> str:\n",
    "        \"\"\"获取记忆摘要\"\"\"\n",
    "        return self.memory.buffer if hasattr(self.memory, 'buffer') else \"暂无对话记录\"\n",
    "\n",
    "# 创建智能助手\n",
    "assistant = SmartConversationAssistant()\n",
    "\n",
    "print(\"=== 智能对话助手测试 ===\")\n",
    "\n",
    "# 测试对话场景\n",
    "test_conversations = [\n",
    "    \"你好，我是小明，我想学习Python编程\",\n",
    "    \"我对机器学习很感兴趣，但不知道从哪里开始\",\n",
    "    \"你还记得我的名字吗？我想学习什么？\",\n",
    "    \"能给我推荐一些Python机器学习的学习路径吗？\",\n",
    "    \"谢谢你的建议，我会按照这个路径学习的\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(test_conversations, 1):\n",
    "    print(f\"\\n--- 对话 {i} ---\")\n",
    "    print(f\"用户: {msg}\")\n",
    "    response = assistant.chat(msg)\n",
    "    print(f\"助手: {response[:200]}...\")\n",
    "\n",
    "print(f\"\\n=== 对话记忆摘要 ===\")\n",
    "print(assistant.get_memory_summary()[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 今日学习总结\n",
    "\n",
    "### ✅ 掌握的核心技能\n",
    "\n",
    "1. **Prompt Templates**\n",
    "   - 基础模板设计\n",
    "   - 条件化提示词\n",
    "   - 角色和风格控制\n",
    "\n",
    "2. **Output Parsers**\n",
    "   - JSON结构化输出\n",
    "   - Pydantic类型安全解析\n",
    "   - 自定义输出格式\n",
    "\n",
    "3. **Memory Systems**\n",
    "   - ConversationBufferMemory（完整记忆）\n",
    "   - ConversationBufferWindowMemory（滑动窗口）\n",
    "   - ConversationSummaryMemory（智能摘要）\n",
    "\n",
    "4. **Document Processing**\n",
    "   - 文档加载和处理\n",
    "   - 多种文本分割策略\n",
    "   - 块大小和重叠优化\n",
    "\n",
    "### 🎯 关键收获\n",
    "\n",
    "- **LangChain的模块化设计**：每个组件都可以独立使用和组合\n",
    "- **链式调用的威力**：`prompt | llm | parser` 的简洁语法\n",
    "- **Memory让对话更智能**：记住上下文，提供连贯体验\n",
    "- **结构化输出的重要性**：便于后续处理和集成\n",
    "\n",
    "### 📝 明日预告：第3天 - RAG核心技术\n",
    "\n",
    "- 向量数据库深入实践\n",
    "- 文档嵌入和相似度搜索\n",
    "- 构建完整的RAG系统\n",
    "- 检索质量优化技巧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
