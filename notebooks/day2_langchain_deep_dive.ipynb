{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬2å¤©ï¼šLangChainæ¡†æ¶æ·±å…¥å­¦ä¹ \n",
    "\n",
    "## ä»Šæ—¥å­¦ä¹ ç›®æ ‡\n",
    "1. æŒæ¡LangChainçš„æ ¸å¿ƒæ¶æ„\n",
    "2. æ·±å…¥ç†è§£Prompt Templateså’ŒOutput Parsers\n",
    "3. å®ç°å¤šç§Memoryç³»ç»Ÿ\n",
    "4. å­¦ä¹ Documentå¤„ç†æµç¨‹\n",
    "5. æ„å»ºä¸€ä¸ªå®Œæ•´çš„å¯¹è¯ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ç¯å¢ƒè®¾ç½®\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LangChainæ ¸å¿ƒæ¶æ„ç†è§£\n",
    "\n",
    "### LangChainçš„6å¤§æ ¸å¿ƒç»„ä»¶\n",
    "1. **Modelsï¼ˆæ¨¡å‹ï¼‰**ï¼šå„ç§LLMçš„ç»Ÿä¸€æ¥å£\n",
    "2. **Promptsï¼ˆæç¤ºè¯ï¼‰**ï¼šç®¡ç†å’Œä¼˜åŒ–æç¤ºè¯\n",
    "3. **Chainsï¼ˆé“¾ï¼‰**ï¼šç»„åˆå¤šä¸ªç»„ä»¶çš„å·¥ä½œæµ\n",
    "4. **Memoryï¼ˆè®°å¿†ï¼‰**ï¼šç»´æŒå¯¹è¯çŠ¶æ€å’Œå†å²\n",
    "5. **Agentsï¼ˆä»£ç†ï¼‰**ï¼šè®©LLMé€‰æ‹©å’Œæ‰§è¡Œå·¥å…·\n",
    "6. **Callbacksï¼ˆå›è°ƒï¼‰**ï¼šç›‘æ§å’Œè°ƒè¯•æ‰§è¡Œè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æ ¸å¿ƒå¯¼å…¥\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "from langchain.schema import BaseMessage\n",
    "from typing import List\n",
    "\n",
    "# åˆå§‹åŒ–åŸºç¡€æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– LLMæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templatesæ·±å…¥å®è·µ\n",
    "\n",
    "### 2.1 åŸºç¡€Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# åŸºç¡€æç¤ºè¯æ¨¡æ¿\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ï¼Œæ“…é•¿{skill}ã€‚è¯·ç”¨{style}çš„é£æ ¼å›ç­”é—®é¢˜ã€‚\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# åˆ›å»ºåŸºç¡€é“¾\n",
    "basic_chain = basic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•ä¸åŒè§’è‰²\n",
    "print(\"=== åŸºç¡€Prompt Templateæµ‹è¯• ===\")\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"role\": \"AIæŠ€æœ¯ä¸“å®¶\",\n",
    "        \"skill\": \"æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ \",\n",
    "        \"style\": \"é€šä¿—æ˜“æ‡‚\",\n",
    "        \"question\": \"ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿ\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"è¯—äºº\",\n",
    "        \"skill\": \"ç°ä»£è¯—åˆ›ä½œ\",\n",
    "        \"style\": \"ä¼˜ç¾æŠ’æƒ…\",\n",
    "        \"question\": \"æè¿°ä¸€ä¸‹æ˜¥å¤©çš„æ™¯è‰²\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n--- æµ‹è¯• {i} ---\")\n",
    "    response = basic_chain.invoke(case)\n",
    "    print(f\"è§’è‰²: {case['role']}\")\n",
    "    print(f\"å›ç­”: {response[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ¡ä»¶åŒ–Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# æ¡ä»¶åŒ–æç¤ºè¯ï¼ˆæ ¹æ®ç”¨æˆ·ç±»å‹è°ƒæ•´ï¼‰\n",
    "def create_conditional_prompt(user_type: str) -> ChatPromptTemplate:\n",
    "    \"\"\"æ ¹æ®ç”¨æˆ·ç±»å‹åˆ›å»ºä¸åŒçš„æç¤ºè¯\"\"\"\n",
    "    \n",
    "    if user_type == \"beginner\":\n",
    "        system_message = \"\"\"\n",
    "        ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„AIå¯¼å¸ˆï¼Œä¸“é—¨ä¸ºåˆå­¦è€…è§£ç­”é—®é¢˜ã€‚\n",
    "        è¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šï¼Œé¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€‚\n",
    "        å¦‚æœå¿…é¡»ä½¿ç”¨æœ¯è¯­ï¼Œè¯·æä¾›ç®€å•çš„è§£é‡Šã€‚\n",
    "        ç”¨ç±»æ¯”å’Œä¾‹å­å¸®åŠ©ç†è§£ã€‚\n",
    "        \"\"\"\n",
    "    elif user_type == \"expert\":\n",
    "        system_message = \"\"\"\n",
    "        ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶ï¼Œä¸ºæœ‰ç»éªŒçš„å¼€å‘è€…æä¾›æ·±å…¥åˆ†æã€‚\n",
    "        å¯ä»¥ä½¿ç”¨ä¸“ä¸šæœ¯è¯­å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚\n",
    "        æä¾›ä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚\n",
    "        å…³æ³¨æ€§èƒ½ä¼˜åŒ–å’Œæ¶æ„è®¾è®¡ã€‚\n",
    "        \"\"\"\n",
    "    else:  # intermediate\n",
    "        system_message = \"\"\"\n",
    "        ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„æŠ€æœ¯é¡¾é—®ã€‚\n",
    "        æä¾›ä¸­ç­‰æ·±åº¦çš„æŠ€æœ¯è§£é‡Šã€‚\n",
    "        åŒ…å«ä¸€äº›ä»£ç ç¤ºä¾‹å’Œå®è·µå»ºè®®ã€‚\n",
    "        å¹³è¡¡ç†è®ºå’Œå®è·µåº”ç”¨ã€‚\n",
    "        \"\"\"\n",
    "    \n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_message.strip()),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "# æµ‹è¯•æ¡ä»¶åŒ–æç¤ºè¯\n",
    "print(\"=== æ¡ä»¶åŒ–Prompt Templateæµ‹è¯• ===\")\n",
    "question = \"ä»€ä¹ˆæ˜¯RESTful APIï¼Ÿ\"\n",
    "\n",
    "for user_type in [\"beginner\", \"intermediate\", \"expert\"]:\n",
    "    print(f\"\\n--- {user_type.upper()} ç‰ˆæœ¬ ---\")\n",
    "    prompt = create_conditional_prompt(user_type)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(f\"å›ç­”: {response[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output Parsersï¼šç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "### 3.1 JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# JSONè¾“å‡ºè§£æå™¨\n",
    "json_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æŠ€æœ¯æ¦‚å¿µï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚\n",
    "    \n",
    "    è¿”å›æ ¼å¼ï¼š\n",
    "    {\n",
    "        \"concept\": \"æŠ€æœ¯æ¦‚å¿µåç§°\",\n",
    "        \"definition\": \"ç®€çŸ­å®šä¹‰\",\n",
    "        \"pros\": [\"ä¼˜ç‚¹1\", \"ä¼˜ç‚¹2\"],\n",
    "        \"cons\": [\"ç¼ºç‚¹1\", \"ç¼ºç‚¹2\"],\n",
    "        \"use_cases\": [\"ä½¿ç”¨åœºæ™¯1\", \"ä½¿ç”¨åœºæ™¯2\"],\n",
    "        \"difficulty\": \"easy/medium/hard\",\n",
    "        \"learning_time\": \"é¢„ä¼°å­¦ä¹ æ—¶é—´\"\n",
    "    }\n",
    "    \"\"\"),\n",
    "    (\"human\", \"è¯·åˆ†æï¼š{concept}\")\n",
    "])\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_chain = json_prompt | llm | json_parser\n",
    "\n",
    "# æµ‹è¯•JSONè§£æ\n",
    "print(\"=== JSON Output Parseræµ‹è¯• ===\")\n",
    "concepts = [\"Dockerå®¹å™¨åŒ–\", \"GraphQL\"]\n",
    "\n",
    "for concept in concepts:\n",
    "    print(f\"\\n--- åˆ†æï¼š{concept} ---\")\n",
    "    try:\n",
    "        result = json_chain.invoke({\"concept\": concept})\n",
    "        print(f\"æ¦‚å¿µ: {result['concept']}\")\n",
    "        print(f\"å®šä¹‰: {result['definition']}\")\n",
    "        print(f\"éš¾åº¦: {result['difficulty']}\")\n",
    "        print(f\"å­¦ä¹ æ—¶é—´: {result['learning_time']}\")\n",
    "        print(f\"ä¼˜ç‚¹: {', '.join(result['pros'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"è§£æé”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pydantic Output Parserï¼ˆç±»å‹å®‰å…¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# å®šä¹‰Pydanticæ¨¡å‹\n",
    "class TechAnalysis(BaseModel):\n",
    "    \"\"\"æŠ€æœ¯åˆ†æç»“æœæ¨¡å‹\"\"\"\n",
    "    concept: str = Field(description=\"æŠ€æœ¯æ¦‚å¿µåç§°\")\n",
    "    definition: str = Field(description=\"æŠ€æœ¯å®šä¹‰\")\n",
    "    pros: List[str] = Field(description=\"ä¼˜ç‚¹åˆ—è¡¨\")\n",
    "    cons: List[str] = Field(description=\"ç¼ºç‚¹åˆ—è¡¨\")\n",
    "    difficulty: str = Field(description=\"éš¾åº¦ç­‰çº§ï¼šeasy/medium/hard\")\n",
    "    learning_time: str = Field(description=\"é¢„ä¼°å­¦ä¹ æ—¶é—´\")\n",
    "    related_technologies: List[str] = Field(description=\"ç›¸å…³æŠ€æœ¯\")\n",
    "\n",
    "# åˆ›å»ºPydanticè§£æå™¨\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=TechAnalysis)\n",
    "\n",
    "pydantic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„æŠ€æœ¯æ¦‚å¿µã€‚\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"è¯·è¯¦ç»†åˆ†æï¼š{concept}\")\n",
    "])\n",
    "\n",
    "pydantic_chain = pydantic_prompt | llm | pydantic_parser\n",
    "\n",
    "# æµ‹è¯•Pydanticè§£æ\n",
    "print(\"=== Pydantic Output Parseræµ‹è¯• ===\")\n",
    "concept = \"å¾®æœåŠ¡æ¶æ„\"\n",
    "\n",
    "try:\n",
    "    result = pydantic_chain.invoke({\n",
    "        \"concept\": concept,\n",
    "        \"format_instructions\": pydantic_parser.get_format_instructions()\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“‹ åˆ†æç»“æœç±»å‹: {type(result)}\")\n",
    "    print(f\"ğŸ¯ æ¦‚å¿µ: {result.concept}\")\n",
    "    print(f\"ğŸ“ å®šä¹‰: {result.definition}\")\n",
    "    print(f\"â­ ä¼˜ç‚¹: {result.pros}\")\n",
    "    print(f\"âš ï¸ ç¼ºç‚¹: {result.cons}\")\n",
    "    print(f\"ğŸšï¸ éš¾åº¦: {result.difficulty}\")\n",
    "    print(f\"â° å­¦ä¹ æ—¶é—´: {result.learning_time}\")\n",
    "    print(f\"ğŸ”— ç›¸å…³æŠ€æœ¯: {result.related_technologies}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"è§£æé”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memoryç³»ç»Ÿï¼šè®©AIè®°ä½å¯¹è¯\n",
    "\n",
    "### 4.1 ConversationBufferMemoryï¼ˆå®Œæ•´è®°å¿†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# åˆ›å»ºå®Œæ•´è®°å¿†ç³»ç»Ÿ\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=buffer_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=== ConversationBufferMemoryæµ‹è¯• ===\")\n",
    "\n",
    "# æ¨¡æ‹Ÿå¯¹è¯\n",
    "conversation_topics = [\n",
    "    \"æˆ‘çš„åå­—æ˜¯å¼ ä¸‰ï¼Œæˆ‘æ˜¯ä¸€åPythonå¼€å‘è€…\",\n",
    "    \"æˆ‘æ­£åœ¨å­¦ä¹ AIæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯LangChain\",\n",
    "    \"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’ŒèŒä¸šå—ï¼Ÿ\",\n",
    "    \"æˆ‘ä¹‹å‰æåˆ°æˆ‘åœ¨å­¦ä¹ ä»€ä¹ˆæŠ€æœ¯ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for i, topic in enumerate(conversation_topics, 1):\n",
    "    print(f\"\\n--- å¯¹è¯è½®æ¬¡ {i} ---\")\n",
    "    print(f\"ç”¨æˆ·: {topic}\")\n",
    "    response = conversation.predict(input=topic)\n",
    "    print(f\"AI: {response}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè®°å¿†å†…å®¹\n",
    "    print(f\"\\nå½“å‰è®°å¿†:\")\n",
    "    print(buffer_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ConversationBufferWindowMemoryï¼ˆæ»‘åŠ¨çª—å£è®°å¿†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# åˆ›å»ºæ»‘åŠ¨çª—å£è®°å¿†ï¼ˆåªè®°ä½æœ€è¿‘3è½®å¯¹è¯ï¼‰\n",
    "window_memory = ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "window_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== ConversationBufferWindowMemoryæµ‹è¯• ===\")\n",
    "print(\"åªè®°ä½æœ€è¿‘3è½®å¯¹è¯\")\n",
    "\n",
    "# è¿›è¡Œå¤šè½®å¯¹è¯æµ‹è¯•\n",
    "test_messages = [\n",
    "    \"ç¬¬1è½®ï¼šæˆ‘å–œæ¬¢è“è‰²\",\n",
    "    \"ç¬¬2è½®ï¼šæˆ‘çš„çˆ±å¥½æ˜¯ç¼–ç¨‹\", \n",
    "    \"ç¬¬3è½®ï¼šæˆ‘ä½åœ¨åŒ—äº¬\",\n",
    "    \"ç¬¬4è½®ï¼šæˆ‘ä»Šå¹´25å²\",\n",
    "    \"ç¬¬5è½®ï¼šæˆ‘å­¦è¿‡Java\",\n",
    "    \"ç¬¬6è½®ï¼šä½ è¿˜è®°å¾—æˆ‘å–œæ¬¢ä»€ä¹ˆé¢œè‰²å—ï¼Ÿ\"  # åº”è¯¥ä¸è®°å¾—ï¼Œå› ä¸ºè¶…å‡ºçª—å£\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    response = window_conversation.predict(input=msg)\n",
    "    print(f\"\\n{msg}\")\n",
    "    print(f\"AI: {response[:100]}...\")\n",
    "    print(f\"è®°å¿†è½®æ¬¡: {len(window_memory.chat_memory.messages)//2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ConversationSummaryMemoryï¼ˆæ‘˜è¦è®°å¿†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# åˆ›å»ºæ‘˜è¦è®°å¿†ç³»ç»Ÿ\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=1000  # å½“è®°å¿†è¶…è¿‡1000 tokensæ—¶å¼€å§‹æ‘˜è¦\n",
    ")\n",
    "\n",
    "summary_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== ConversationSummaryMemoryæµ‹è¯• ===\")\n",
    "\n",
    "# æ¨¡æ‹Ÿé•¿å¯¹è¯\n",
    "long_conversation = [\n",
    "    \"æˆ‘æ˜¯ä¸€åAIç ”ç©¶å‘˜ï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†\",\n",
    "    \"æˆ‘æœ€è¿‘åœ¨ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæŠ€æœ¯\",\n",
    "    \"æˆ‘å‘ç°LoRAæ˜¯ä¸€ä¸ªå¾ˆæœ‰æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•\",\n",
    "    \"æˆ‘è¿˜å°è¯•è¿‡QLoRAï¼Œå®ƒåœ¨å†…å­˜ä½¿ç”¨ä¸Šæ›´ä¼˜åŒ–\",\n",
    "    \"é™¤äº†å¾®è°ƒï¼Œæˆ‘ä¹Ÿåœ¨ç ”ç©¶RAGæŠ€æœ¯\",\n",
    "    \"RAGå¯ä»¥è®©æ¨¡å‹è·å–æœ€æ–°çš„å¤–éƒ¨çŸ¥è¯†\",\n",
    "    \"ä½ èƒ½æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„å†…å®¹å—ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(long_conversation, 1):\n",
    "    response = summary_conversation.predict(input=msg)\n",
    "    print(f\"\\nè½®æ¬¡ {i}: {msg}\")\n",
    "    print(f\"AI: {response[:150]}...\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ‘˜è¦\n",
    "    if hasattr(summary_memory, 'buffer'):\n",
    "        print(f\"æ‘˜è¦: {summary_memory.buffer[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Loaderså’ŒText Splitters\n",
    "\n",
    "### 5.1 å¤„ç†ä¸åŒæ ¼å¼çš„æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter, \n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ–‡æ¡£\n",
    "sample_text = \"\"\"\n",
    "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\n",
    "\n",
    "æœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚æœºå™¨å­¦ä¹ ç®—æ³•ä½¿ç”¨ç»Ÿè®¡æŠ€æœ¯æ¥æ‰¾åˆ°æ•°æ®ä¸­çš„æ¨¡å¼ã€‚\n",
    "\n",
    "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learning, DLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\n",
    "\n",
    "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing, NLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPç»“åˆäº†è®¡ç®—è¯­è¨€å­¦ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚\n",
    "\n",
    "å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰å¦‚GPTã€BERTç­‰ï¼Œä»£è¡¨äº†NLPé¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚è¿™äº›æ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆé«˜è´¨é‡çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# åˆ›å»ºæ–‡æ¡£å¯¹è±¡\n",
    "doc = Document(page_content=sample_text, metadata={\"source\": \"AIæŠ€æœ¯æ¦‚è¿°\"})\n",
    "\n",
    "print(\"=== æ–‡æ¡£å¤„ç†æµ‹è¯• ===\")\n",
    "print(f\"åŸå§‹æ–‡æ¡£é•¿åº¦: {len(sample_text)} å­—ç¬¦\")\n",
    "print(f\"åŸå§‹æ–‡æ¡£:\\n{sample_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ä¸åŒçš„æ–‡æœ¬åˆ†å‰²ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # æ¯å—æœ€å¤§å­—ç¬¦æ•°\n",
    "    chunk_overlap=50,  # é‡å å­—ç¬¦æ•°\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \";\", \",\", \" \", \"\"]  # åˆ†å‰²ä¼˜å…ˆçº§\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_documents([doc])\n",
    "\n",
    "print(\"\\n=== é€’å½’å­—ç¬¦åˆ†å‰²å™¨ ===\")\n",
    "print(f\"åˆ†å‰²åå—æ•°: {len(recursive_chunks)}\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"\\nå— {i+1} (é•¿åº¦: {len(chunk.page_content)})\")\n",
    "    print(f\"{chunk.page_content[:150]}...\")\n",
    "\n",
    "# 2. åŸºäºTokençš„åˆ†å‰²å™¨\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # æ¯å—æœ€å¤§tokenæ•°\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"\\n=== Tokenåˆ†å‰²å™¨ ===\")\n",
    "print(f\"åˆ†å‰²åå—æ•°: {len(token_chunks)}\")\n",
    "for i, chunk in enumerate(token_chunks[:2]):  # åªæ˜¾ç¤ºå‰ä¸¤å—\n",
    "    print(f\"\\nå— {i+1}:\")\n",
    "    print(f\"{chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç»¼åˆå®è·µï¼šæ„å»ºæ™ºèƒ½å¯¹è¯åŠ©æ‰‹\n",
    "\n",
    "å°†ä»Šå¤©å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯æ•´åˆåˆ°ä¸€ä¸ªå®Œæ•´çš„å¯¹è¯ç³»ç»Ÿä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SmartConversationAssistant:\n",
    "    \"\"\"æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ - æ•´åˆä»Šæ—¥æ‰€å­¦æŠ€æœ¯\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "        self.memory = ConversationBufferWindowMemory(k=5)  # è®°ä½æœ€è¿‘5è½®å¯¹è¯\n",
    "        \n",
    "        # æ™ºèƒ½æç¤ºè¯æ¨¡æ¿\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "            ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š\n",
    "            1. è®°ä½å¯¹è¯å†å²ï¼Œæä¾›è¿è´¯çš„å›ç­”\n",
    "            2. æ ¹æ®ç”¨æˆ·é—®é¢˜ç±»å‹è°ƒæ•´å›ç­”é£æ ¼\n",
    "            3. æä¾›ç»“æ„åŒ–çš„ä¿¡æ¯åˆ†æ\n",
    "            \n",
    "            å¯¹è¯å†å²ï¼š{history}\n",
    "            \n",
    "            è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„å›ç­”æ–¹å¼ï¼š\n",
    "            - æŠ€æœ¯é—®é¢˜ï¼šæä¾›è¯¦ç»†è§£é‡Šå’Œä»£ç ç¤ºä¾‹\n",
    "            - æ¦‚å¿µè§£é‡Šï¼šç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€å’Œç±»æ¯”\n",
    "            - ä¸ªäººå¯¹è¯ï¼šè®°ä½ç”¨æˆ·ä¿¡æ¯ï¼Œæä¾›ä¸ªæ€§åŒ–å›ç­”\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # åˆ›å»ºå¯¹è¯é“¾\n",
    "        self.conversation = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=self.prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"è¿›è¡Œå¯¹è¯\"\"\"\n",
    "        try:\n",
    "            response = self.conversation.predict(input=message)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™ï¼š{str(e)}\"\n",
    "    \n",
    "    def get_memory_summary(self) -> str:\n",
    "        \"\"\"è·å–è®°å¿†æ‘˜è¦\"\"\"\n",
    "        return self.memory.buffer if hasattr(self.memory, 'buffer') else \"æš‚æ— å¯¹è¯è®°å½•\"\n",
    "\n",
    "# åˆ›å»ºæ™ºèƒ½åŠ©æ‰‹\n",
    "assistant = SmartConversationAssistant()\n",
    "\n",
    "print(\"=== æ™ºèƒ½å¯¹è¯åŠ©æ‰‹æµ‹è¯• ===\")\n",
    "\n",
    "# æµ‹è¯•å¯¹è¯åœºæ™¯\n",
    "test_conversations = [\n",
    "    \"ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹\",\n",
    "    \"æˆ‘å¯¹æœºå™¨å­¦ä¹ å¾ˆæ„Ÿå…´è¶£ï¼Œä½†ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹\",\n",
    "    \"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿæˆ‘æƒ³å­¦ä¹ ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"èƒ½ç»™æˆ‘æ¨èä¸€äº›Pythonæœºå™¨å­¦ä¹ çš„å­¦ä¹ è·¯å¾„å—ï¼Ÿ\",\n",
    "    \"è°¢è°¢ä½ çš„å»ºè®®ï¼Œæˆ‘ä¼šæŒ‰ç…§è¿™ä¸ªè·¯å¾„å­¦ä¹ çš„\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(test_conversations, 1):\n",
    "    print(f\"\\n--- å¯¹è¯ {i} ---\")\n",
    "    print(f\"ç”¨æˆ·: {msg}\")\n",
    "    response = assistant.chat(msg)\n",
    "    print(f\"åŠ©æ‰‹: {response[:200]}...\")\n",
    "\n",
    "print(f\"\\n=== å¯¹è¯è®°å¿†æ‘˜è¦ ===\")\n",
    "print(assistant.get_memory_summary()[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ä»Šæ—¥å­¦ä¹ æ€»ç»“\n",
    "\n",
    "### âœ… æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½\n",
    "\n",
    "1. **Prompt Templates**\n",
    "   - åŸºç¡€æ¨¡æ¿è®¾è®¡\n",
    "   - æ¡ä»¶åŒ–æç¤ºè¯\n",
    "   - è§’è‰²å’Œé£æ ¼æ§åˆ¶\n",
    "\n",
    "2. **Output Parsers**\n",
    "   - JSONç»“æ„åŒ–è¾“å‡º\n",
    "   - Pydanticç±»å‹å®‰å…¨è§£æ\n",
    "   - è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼\n",
    "\n",
    "3. **Memory Systems**\n",
    "   - ConversationBufferMemoryï¼ˆå®Œæ•´è®°å¿†ï¼‰\n",
    "   - ConversationBufferWindowMemoryï¼ˆæ»‘åŠ¨çª—å£ï¼‰\n",
    "   - ConversationSummaryMemoryï¼ˆæ™ºèƒ½æ‘˜è¦ï¼‰\n",
    "\n",
    "4. **Document Processing**\n",
    "   - æ–‡æ¡£åŠ è½½å’Œå¤„ç†\n",
    "   - å¤šç§æ–‡æœ¬åˆ†å‰²ç­–ç•¥\n",
    "   - å—å¤§å°å’Œé‡å ä¼˜åŒ–\n",
    "\n",
    "### ğŸ¯ å…³é”®æ”¶è·\n",
    "\n",
    "- **LangChainçš„æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹ä½¿ç”¨å’Œç»„åˆ\n",
    "- **é“¾å¼è°ƒç”¨çš„å¨åŠ›**ï¼š`prompt | llm | parser` çš„ç®€æ´è¯­æ³•\n",
    "- **Memoryè®©å¯¹è¯æ›´æ™ºèƒ½**ï¼šè®°ä½ä¸Šä¸‹æ–‡ï¼Œæä¾›è¿è´¯ä½“éªŒ\n",
    "- **ç»“æ„åŒ–è¾“å‡ºçš„é‡è¦æ€§**ï¼šä¾¿äºåç»­å¤„ç†å’Œé›†æˆ\n",
    "\n",
    "### ğŸ“ æ˜æ—¥é¢„å‘Šï¼šç¬¬3å¤© - RAGæ ¸å¿ƒæŠ€æœ¯\n",
    "\n",
    "- å‘é‡æ•°æ®åº“æ·±å…¥å®è·µ\n",
    "- æ–‡æ¡£åµŒå…¥å’Œç›¸ä¼¼åº¦æœç´¢\n",
    "- æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ\n",
    "- æ£€ç´¢è´¨é‡ä¼˜åŒ–æŠ€å·§"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"